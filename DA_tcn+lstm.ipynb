{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b831c663",
   "metadata": {},
   "source": [
    "## Importing libraries and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec64270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "import TCN.TCN.tcn as tcn\n",
    "from TCN.TCN.tcn import TemporalConvNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f9b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            store  item  sales\n",
       "date                          \n",
       "2013-01-01      1     1     13\n",
       "2013-01-02      1     1     11\n",
       "2013-01-03      1     1     14\n",
       "2013-01-04      1     1     13\n",
       "2013-01-05      1     1     10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', parse_dates=['date'], index_col = ['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd39674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store  item  sales\n",
       "0 2013-01-01      1     1     13\n",
       "1 2013-01-02      1     1     11\n",
       "2 2013-01-03      1     1     14\n",
       "3 2013-01-04      1     1     13\n",
       "4 2013-01-05      1     1     10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df.sort_values('date',ascending=True)\n",
    "test = df[df.index.year == 2017]\n",
    "test.reset_index(level=0, inplace= True)\n",
    "train = df[df.index.year != 2017]\n",
    "train.reset_index(level = 0, inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459ff05",
   "metadata": {},
   "source": [
    "# Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b45341",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'year': train['date'].dt.year-2013, 'month': train['date'].dt.month,\n",
    "                           'day': train['date'].dt.day, 'weekday': train['date'].dt.weekday,\n",
    "                           'store': train['store'], 'item': train['item'], 'sales': train['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n",
    "\n",
    "test_data = pd.DataFrame({'year': test['date'].dt.year-2013, 'month': test['date'].dt.month,\n",
    "                           'day': test['date'].dt.day, 'weekday': test['date'].dt.weekday,\n",
    "                           'store': test['store'], 'item': test['item'], 'sales': test['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc87eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month  day  weekday  store  item  sales\n",
      "0     0      1    1        1      1     1     13\n",
      "1     0      1    2        2      1     1     11\n",
      "2     0      1    3        3      1     1     14\n",
      "3     0      1    4        4      1     1     13\n",
      "4     0      1    5        5      1     1     10\n",
      "(730500, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73fb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_data.drop('sales', axis = 1))\n",
    "y = np.array(train_data['sales'])\n",
    "X_test = np.array(test_data.drop('sales', axis = 1))\n",
    "y_test = np.array(test_data['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d91b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X_train, y_train, val_ratio = 0.2, val_year = 3, half_yearly = 1, randomly = True):\n",
    "    \n",
    "    # Splitting randomly\n",
    "    if randomly:\n",
    "        X_tr, y_tr, X_val, y_val = train_test_split(X_train, y_train, test_size = (val_ratio),\n",
    "                                                          random_state = 6, shuffle = True)\n",
    "    else:\n",
    "        if half_yearly == 1:        #if validation data is first 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]   #if not val_year or in last 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)] #if val_year and first 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)]\n",
    "            \n",
    "        else:                       #if validation data is last 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]  #if not val_year or in first 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]  #if val_year and last 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]\n",
    "            \n",
    "        return X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc559b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (730500, 6) (730500,)\n",
      "Validation: (0, 6) (0,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(X, y, False, 0.3, 3, 0)\n",
    "print(\"Training:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe1d3b",
   "metadata": {},
   "source": [
    "# Creating TCN+LSTM hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899a8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 13, 32, 8, 11, 51]\n",
      "[(5, 3), (13, 7), (32, 16), (8, 4), (11, 6), (51, 26)]\n"
     ]
    }
   ],
   "source": [
    "# For embeddings, the thumb rule is, num_embeddings = no. of unique valus in category + 1 \n",
    "# & embedding_dim = min(50,feat_dim(num_embeddings)/2)\n",
    "dims = [np.unique(X_train[:,i]).size+1 for i in range(X_train.shape[1])]\n",
    "print(dims)\n",
    "embedding_dim = [(x, min(50, (x+1)//2)) for x in dims]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896e1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class for TCN+LSTM hybrid model\n",
    "class LSTMTCNwithEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim,  n_cont, out_size, ker_size, dense_layers,  num_channels = [1], dp = 0.3): #n_cont=no. of cont. feat. in dataframe\n",
    "        super(LSTMTCNwithEmbeddings,self).__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        #print(self.embeds)\n",
    "        self.emb_drop = nn.Dropout(dp)\n",
    "        \n",
    "        layer_list = []\n",
    "        n_emb = sum((out for inp,out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "        n_hidden = [112,96]\n",
    "        #n_dense = [64, 32]\n",
    "        \n",
    "        self.tcn1 = TemporalConvNet(n_in, [112], kernel_size=ker_size, dropout=dp)\n",
    "        #self.lstm1 = nn.LSTM(n_in, n_hidden[0], 1, batch_first = True) #(no. of inputs, hidden_size, num_layers)\n",
    "        self.lstm1 = nn.LSTM(n_hidden[0], n_hidden[1], 1, batch_first = True)\n",
    "        n_in_dense = n_hidden[1]    \n",
    "        for i in dense_layers:\n",
    "            layer_list.append(nn.Linear(n_in_dense, i))\n",
    "            layer_list.append(nn.ReLU(inplace = True))\n",
    "            #layer_list.append(nn.BatchNorm1d(i))\n",
    "            n_in_dense = i\n",
    "            \n",
    "        layer_list.append(nn.Dropout(dp))\n",
    "        layer_list.append(nn.Linear(n_in_dense, out_size))\n",
    "\n",
    "        self.dense_layers = nn.Sequential(*layer_list)\n",
    "\n",
    "\n",
    "    def forward(self, X_cat, X_cont):\n",
    "        embeddings = []\n",
    "        n_hidden = [112, 96]\n",
    "        batch_size = 365\n",
    "        seq_len = 1\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(X_cat[:,i]))\n",
    "        X = torch.cat(embeddings, axis =1)\n",
    "        X = self.emb_drop(X)\n",
    "        X = torch.cat([X, torch.unsqueeze(X_cont,1)], 1)\n",
    "        \n",
    "        X = X.reshape(1,X.size(1),X.size(0))\n",
    "        #print(X.size())\n",
    "        out = self.tcn1(X)\n",
    "        #print(out.size())\n",
    "        h_1 = torch.randn(1, out.size(2), n_hidden[1]) #hidden state  ##(num_layers, batch_size, hidden_size)\n",
    "        c_1 = torch.randn(1, out.size(2), n_hidden[1]) #internal state/cell state\n",
    "        #print(h_1.size(), c_1.size())\n",
    "        h_out1, c_out1 = self.lstm1(out.reshape(out.size(2), 1, out.size(1)), (h_1,c_1)) #((batch_size(no. of elements in batch),seq_len, input_shape),(hidden))\n",
    "        #h_out2, c_out2 = self.lstm2(h_out1, (h_2,c_2))\n",
    "        #print(h_out1.size())\n",
    "        f_out = self.dense_layers(h_out1)\n",
    "        return f_out.reshape(f_out.size(0),-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8649dcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTCNwithEmbeddings(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(13, 7)\n",
       "    (1): Embedding(32, 16)\n",
       "    (2): Embedding(8, 4)\n",
       "    (3): Embedding(11, 6)\n",
       "    (4): Embedding(51, 26)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0, inplace=False)\n",
       "  (tcn1): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(60, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (conv2): Conv1d(112, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(60, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0, inplace=False)\n",
       "          (4): Conv1d(112, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(60, 112, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm1): LSTM(112, 96, batch_first=True)\n",
       "  (dense_layers): Sequential(\n",
       "    (0): Linear(in_features=96, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Dropout(p=0, inplace=False)\n",
       "    (5): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiating TCN+LSTM hybrid model with 1 TCN layer, 1 LSTM layer, 2 Fully connected layers and an output layer \n",
    "# having 112,96,64 & 32 nodes respectively\n",
    "\n",
    "LSTMTCNmodel = LSTMTCNwithEmbeddings(embedding_dim[1:], 1, 1, ker_size=2,dense_layers=[64,32], num_channels=[1], dp=0)\n",
    "LSTMTCNmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0165a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function (optional use)\n",
    "def smape(x,y):\n",
    "    return 100*torch.mean(2*torch.abs(x-y)/(torch.abs(x)+torch.abs(y)))\n",
    "\n",
    "optim = torch.optim.Adam(LSTMTCNmodel.parameters(), lr = 0.01)\n",
    "lossfn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450725e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f50a4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, sets, model, X_train, y_train, lossfn, optimizer):\n",
    "    ep_count = 0\n",
    "    for i in range(sets//2):\n",
    "        for j in range(2):\n",
    "            X_tr, y_tr, X_val, y_val = split_data(X_train, y_train, val_year=i, half_yearly=j, randomly=False)\n",
    "            losses = []\n",
    "            for k in range(epochs):\n",
    "                k+=1\n",
    "                y_pred = LSTMTCNmodel(torch.from_numpy(X_tr[:,1:]), torch.from_numpy(X_tr[:,0]))\n",
    "                y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n",
    "                loss = lossfn(y_pred,torch.tensor(y_tr))\n",
    "                losses.append(loss)\n",
    "                #if k%2 == 1:\n",
    "                print(\"Epoch number {} of validation year 20{} and half {} has MSE loss {}\".format(k,13+i,j,loss.item()))\n",
    "                ep_count+=1\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if k%4 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        yhat_val = LSTMTCNmodel(torch.from_numpy(X_val[:,1:]), torch.from_numpy(X_val[:,0]))\n",
    "                        y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n",
    "                        val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n",
    "                    print(\"Validation loss at epoch {} of year 20{} half {} is {:.4f}\".format(k,13+i,j,val_loss.item()))\n",
    "    \n",
    "    print(\"Total number of epochs is\", ep_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6e7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833d5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9162/317579201.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = lossfn(y_pred,torch.tensor(y_tr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1 of validation year 2013 and half 0 has MSE loss 3420.99560546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9162/317579201.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 2 of validation year 2013 and half 0 has MSE loss 3399.58056640625\n",
      "Epoch number 3 of validation year 2013 and half 0 has MSE loss 3326.6044921875\n",
      "Epoch number 4 of validation year 2013 and half 0 has MSE loss 3177.843994140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9162/317579201.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 4 of year 2013 half 0 is 46.6387\n",
      "Epoch number 5 of validation year 2013 and half 0 has MSE loss 2917.8310546875\n",
      "Epoch number 6 of validation year 2013 and half 0 has MSE loss 2608.613525390625\n",
      "Epoch number 7 of validation year 2013 and half 0 has MSE loss 2237.07763671875\n",
      "Epoch number 8 of validation year 2013 and half 0 has MSE loss 1805.60302734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9162/317579201.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 8 of year 2013 half 0 is 29.7601\n",
      "Epoch number 9 of validation year 2013 and half 0 has MSE loss 1357.64599609375\n",
      "Epoch number 10 of validation year 2013 and half 0 has MSE loss 975.9345703125\n",
      "Epoch number 1 of validation year 2013 and half 1 has MSE loss 797.5214233398438\n",
      "Epoch number 2 of validation year 2013 and half 1 has MSE loss 953.1113891601562\n",
      "Epoch number 3 of validation year 2013 and half 1 has MSE loss 1278.9234619140625\n",
      "Epoch number 4 of validation year 2013 and half 1 has MSE loss 1365.5224609375\n",
      "Validation loss at epoch 4 of year 2013 half 1 is 38.3680\n",
      "Epoch number 5 of validation year 2013 and half 1 has MSE loss 1214.736572265625\n",
      "Epoch number 6 of validation year 2013 and half 1 has MSE loss 1000.3665771484375\n",
      "Epoch number 7 of validation year 2013 and half 1 has MSE loss 847.5330810546875\n",
      "Epoch number 8 of validation year 2013 and half 1 has MSE loss 796.21142578125\n",
      "Validation loss at epoch 8 of year 2013 half 1 is 23.5881\n",
      "Epoch number 9 of validation year 2013 and half 1 has MSE loss 824.6604614257812\n",
      "Epoch number 10 of validation year 2013 and half 1 has MSE loss 889.3994140625\n",
      "Epoch number 1 of validation year 2014 and half 0 has MSE loss 898.46728515625\n",
      "Epoch number 2 of validation year 2014 and half 0 has MSE loss 937.5946044921875\n",
      "Epoch number 3 of validation year 2014 and half 0 has MSE loss 951.733154296875\n",
      "Epoch number 4 of validation year 2014 and half 0 has MSE loss 940.1725463867188\n",
      "Validation loss at epoch 4 of year 2014 half 0 is 30.5832\n",
      "Epoch number 5 of validation year 2014 and half 0 has MSE loss 908.110107421875\n",
      "Epoch number 6 of validation year 2014 and half 0 has MSE loss 864.4142456054688\n",
      "Epoch number 7 of validation year 2014 and half 0 has MSE loss 820.5966186523438\n",
      "Epoch number 8 of validation year 2014 and half 0 has MSE loss 788.3875122070312\n",
      "Validation loss at epoch 8 of year 2014 half 0 is 27.6330\n",
      "Epoch number 9 of validation year 2014 and half 0 has MSE loss 776.4775390625\n",
      "Epoch number 10 of validation year 2014 and half 0 has MSE loss 786.4317016601562\n",
      "Epoch number 1 of validation year 2014 and half 1 has MSE loss 812.4887084960938\n",
      "Epoch number 2 of validation year 2014 and half 1 has MSE loss 834.5728759765625\n",
      "Epoch number 3 of validation year 2014 and half 1 has MSE loss 845.0601196289062\n",
      "Epoch number 4 of validation year 2014 and half 1 has MSE loss 838.7523803710938\n",
      "Validation loss at epoch 4 of year 2014 half 1 is 27.8253\n",
      "Epoch number 5 of validation year 2014 and half 1 has MSE loss 820.7496948242188\n",
      "Epoch number 6 of validation year 2014 and half 1 has MSE loss 801.0241088867188\n",
      "Epoch number 7 of validation year 2014 and half 1 has MSE loss 788.094482421875\n",
      "Epoch number 8 of validation year 2014 and half 1 has MSE loss 785.3806762695312\n",
      "Validation loss at epoch 8 of year 2014 half 1 is 26.3344\n",
      "Epoch number 9 of validation year 2014 and half 1 has MSE loss 790.7850952148438\n",
      "Epoch number 10 of validation year 2014 and half 1 has MSE loss 799.6806030273438\n",
      "Epoch number 1 of validation year 2015 and half 0 has MSE loss 778.9850463867188\n",
      "Epoch number 2 of validation year 2015 and half 0 has MSE loss 781.9315185546875\n",
      "Epoch number 3 of validation year 2015 and half 0 has MSE loss 781.1136474609375\n",
      "Epoch number 4 of validation year 2015 and half 0 has MSE loss 777.209716796875\n",
      "Validation loss at epoch 4 of year 2015 half 0 is 29.5750\n",
      "Epoch number 5 of validation year 2015 and half 0 has MSE loss 771.9176635742188\n",
      "Epoch number 6 of validation year 2015 and half 0 has MSE loss 767.3366088867188\n",
      "Epoch number 7 of validation year 2015 and half 0 has MSE loss 765.1629028320312\n",
      "Epoch number 8 of validation year 2015 and half 0 has MSE loss 765.6973266601562\n",
      "Validation loss at epoch 8 of year 2015 half 0 is 28.8427\n",
      "Epoch number 9 of validation year 2015 and half 0 has MSE loss 768.185791015625\n",
      "Epoch number 10 of validation year 2015 and half 0 has MSE loss 770.8292236328125\n",
      "Epoch number 1 of validation year 2015 and half 1 has MSE loss 782.0712890625\n",
      "Epoch number 2 of validation year 2015 and half 1 has MSE loss 781.9337768554688\n",
      "Epoch number 3 of validation year 2015 and half 1 has MSE loss 780.7606201171875\n",
      "Epoch number 4 of validation year 2015 and half 1 has MSE loss 779.274658203125\n",
      "Validation loss at epoch 4 of year 2015 half 1 is 27.4229\n",
      "Epoch number 5 of validation year 2015 and half 1 has MSE loss 778.3576049804688\n",
      "Epoch number 6 of validation year 2015 and half 1 has MSE loss 778.1327514648438\n",
      "Epoch number 7 of validation year 2015 and half 1 has MSE loss 778.532470703125\n",
      "Epoch number 8 of validation year 2015 and half 1 has MSE loss 779.2745361328125\n",
      "Validation loss at epoch 8 of year 2015 half 1 is 27.4039\n",
      "Epoch number 9 of validation year 2015 and half 1 has MSE loss 779.7203979492188\n",
      "Epoch number 10 of validation year 2015 and half 1 has MSE loss 779.775634765625\n",
      "Epoch number 1 of validation year 2016 and half 0 has MSE loss 735.14697265625\n",
      "Epoch number 2 of validation year 2016 and half 0 has MSE loss 735.1629638671875\n",
      "Epoch number 3 of validation year 2016 and half 0 has MSE loss 735.245361328125\n",
      "Epoch number 4 of validation year 2016 and half 0 has MSE loss 735.2601928710938\n",
      "Validation loss at epoch 4 of year 2016 half 0 is 32.4801\n",
      "Epoch number 5 of validation year 2016 and half 0 has MSE loss 735.2191772460938\n",
      "Epoch number 6 of validation year 2016 and half 0 has MSE loss 735.15869140625\n",
      "Epoch number 7 of validation year 2016 and half 0 has MSE loss 735.136474609375\n",
      "Epoch number 8 of validation year 2016 and half 0 has MSE loss 735.1015014648438\n",
      "Validation loss at epoch 8 of year 2016 half 0 is 32.6102\n",
      "Epoch number 9 of validation year 2016 and half 0 has MSE loss 735.0580444335938\n",
      "Epoch number 10 of validation year 2016 and half 0 has MSE loss 735.0885620117188\n",
      "Epoch number 1 of validation year 2016 and half 1 has MSE loss 759.8251953125\n",
      "Epoch number 2 of validation year 2016 and half 1 has MSE loss 759.6482543945312\n",
      "Epoch number 3 of validation year 2016 and half 1 has MSE loss 759.2610473632812\n",
      "Epoch number 4 of validation year 2016 and half 1 has MSE loss 758.9832763671875\n",
      "Validation loss at epoch 4 of year 2016 half 1 is 29.8033\n",
      "Epoch number 5 of validation year 2016 and half 1 has MSE loss 758.8165283203125\n",
      "Epoch number 6 of validation year 2016 and half 1 has MSE loss 758.9683227539062\n",
      "Epoch number 7 of validation year 2016 and half 1 has MSE loss 759.1143798828125\n",
      "Epoch number 8 of validation year 2016 and half 1 has MSE loss 759.271484375\n",
      "Validation loss at epoch 8 of year 2016 half 1 is 29.7188\n",
      "Epoch number 9 of validation year 2016 and half 1 has MSE loss 759.2708740234375\n",
      "Epoch number 10 of validation year 2016 and half 1 has MSE loss 759.1749267578125\n",
      "Total number of epochs is 80\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "fit(10, 8, LSTMTCNmodel, X, y, lossfn, optim)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "682b3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training in a Ryzen 5 Hexa core 4600H CPU is 19.70 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken for training in a Ryzen 5 Hexa core 4600H CPU is {:.2f} minutes\".format((end-begin)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6ed21",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c036cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "#import torch_metrics as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bdd7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(test, y_pred_final):\n",
    "    metrics = {'R2_score': r2_score(test, y_pred_final), 'MAE': mean_absolute_error(test, y_pred_final),\n",
    "               'RMSE': mean_squared_error(test, y_pred_final, squared=False),\n",
    "               'MAPE': mean_absolute_percentage_error(test, y_pred_final)}\n",
    "    adj_R2 = 1-(1-metrics['R2_score'])*(len(test)-1)/(len(test)-6-1)      #num of indep var = 6\n",
    "    metrics['adj_R2'] = adj_R2\n",
    "    print(\"R2 score on test set is\", metrics['R2_score'])\n",
    "    print(\"Mean Absolute Error on test set is\", metrics['MAE'])\n",
    "    print(\"Root Mean Square error on test set is\", metrics['RMSE'])\n",
    "    print(\"Mean Absolute Percentage Error on test set is\", metrics['MAPE'])\n",
    "    print(\"Adjusted R2 score on test set is\", adj_R2)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52181b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on test set is -0.0698741653697148\n",
      "Mean Absolute Error on test set is 25.29713485937249\n",
      "Root Mean Square error on test set is 32.63635535486093\n",
      "Mean Absolute Percentage Error on test set is 0.5577620565618376\n",
      "Adjusted R2 score on test set is -0.06990934066406718\n"
     ]
    }
   ],
   "source": [
    "predictions = LSTMTCNmodel(torch.from_numpy(X_test[:,1:]), torch.from_numpy(X_test[:,0]))\n",
    "test_results = show_metrics(y_test, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb10a8c",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9742295",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(LSTMTCNmodel.state_dict(), 'TCN+LSTM.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
