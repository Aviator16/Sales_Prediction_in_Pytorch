{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12c815f",
   "metadata": {},
   "source": [
    "## Cloning TCN module for pytorch, importing modules and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3c2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/locuslab/TCN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec64270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "import TCN.TCN.tcn as tcn\n",
    "from TCN.TCN.tcn import TemporalConvNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f9b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            store  item  sales\n",
       "date                          \n",
       "2013-01-01      1     1     13\n",
       "2013-01-02      1     1     11\n",
       "2013-01-03      1     1     14\n",
       "2013-01-04      1     1     13\n",
       "2013-01-05      1     1     10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', parse_dates=['date'], index_col = ['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd39674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store  item  sales\n",
       "0 2013-01-01      1     1     13\n",
       "1 2013-01-02      1     1     11\n",
       "2 2013-01-03      1     1     14\n",
       "3 2013-01-04      1     1     13\n",
       "4 2013-01-05      1     1     10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df.sort_values('date',ascending=True)\n",
    "test = df[df.index.year == 2017]\n",
    "test.reset_index(level=0, inplace= True)\n",
    "train = df[df.index.year != 2017]\n",
    "train.reset_index(level = 0, inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459ff05",
   "metadata": {},
   "source": [
    "# Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b45341",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'year': train['date'].dt.year-2013, 'month': train['date'].dt.month,\n",
    "                           'day': train['date'].dt.day, 'weekday': train['date'].dt.weekday,\n",
    "                           'store': train['store'], 'item': train['item'], 'sales': train['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n",
    "\n",
    "test_data = pd.DataFrame({'year': test['date'].dt.year-2013, 'month': test['date'].dt.month,\n",
    "                           'day': test['date'].dt.day, 'weekday': test['date'].dt.weekday,\n",
    "                           'store': test['store'], 'item': test['item'], 'sales': test['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc87eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month  day  weekday  store  item  sales\n",
      "0     0      1    1        1      1     1     13\n",
      "1     0      1    2        2      1     1     11\n",
      "2     0      1    3        3      1     1     14\n",
      "3     0      1    4        4      1     1     13\n",
      "4     0      1    5        5      1     1     10\n",
      "(730500, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73fb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_data.drop('sales', axis = 1))\n",
    "y = np.array(train_data['sales'])\n",
    "X_test = np.array(test_data.drop('sales', axis = 1))\n",
    "y_test = np.array(test_data['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d91b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X_train, y_train, val_ratio = 0.2, val_year = 3, half_yearly = 1, randomly = True):\n",
    "    \n",
    "    # Splitting randomly\n",
    "    if randomly:\n",
    "        X_tr, y_tr, X_val, y_val = train_test_split(X_train, y_train, test_size = (val_ratio),\n",
    "                                                          random_state = 6, shuffle = True)\n",
    "    else:\n",
    "        if half_yearly == 1:        #if validation data is first 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]   #if not val_year or in last 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)] #if val_year and first 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)]\n",
    "            \n",
    "        else:                       #if validation data is last 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]  #if not val_year or in first 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]  #if val_year and last 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]\n",
    "            \n",
    "        return X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc559b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (730500, 6) (730500,)\n",
      "Validation: (0, 6) (0,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(X, y, False, 0.3, 3, 0)\n",
    "print(\"Training:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe1d3b",
   "metadata": {},
   "source": [
    "# Creating Temporal Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "899a8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 13, 32, 8, 11, 51]\n",
      "[(5, 3), (13, 7), (32, 16), (8, 4), (11, 6), (51, 26)]\n"
     ]
    }
   ],
   "source": [
    "# For embeddings, the thumb rule is, num_embeddings = no. of unique valus in category + 1 \n",
    "# & embedding_dim = min(50,feat_dim(num_embeddings)/2)\n",
    "dims = [np.unique(X_train[:,i]).size+1 for i in range(X_train.shape[1])]\n",
    "print(dims)\n",
    "embedding_dim = [(x, min(50, (x+1)//2)) for x in dims]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896e1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class for TCN\n",
    "class TCNwithEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_cont, out_size, ker_size, dense_layers,  num_channels = [1], dp = 0.3):    #n_cont=no. of cont. feat. in dataframe\n",
    "        super(TCNwithEmbeddings,self).__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        self.emb_drop = nn.Dropout(dp)\n",
    "        \n",
    "        layer_list = []\n",
    "        n_emb = sum((out for inp,out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "        self.tcn1 = TemporalConvNet(n_in, [112], kernel_size=ker_size, dropout=dp)\n",
    "        self.tcn2 = TemporalConvNet(112, [96], kernel_size=ker_size, dropout=dp)\n",
    "        inp_s = 96\n",
    "        for i in dense_layers:\n",
    "            layer_list.append(nn.Linear(inp_s, i))\n",
    "            layer_list.append(nn.ReLU(inplace = True))\n",
    "            #layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(dp))\n",
    "            inp_s = i\n",
    "        layer_list.append(nn.Linear(dense_layers[-1], out_size))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "        \n",
    "    def forward(self, X_cat, X_cont):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(X_cat[:,i]))\n",
    "        X = torch.cat(embeddings, axis =1)\n",
    "        X = self.emb_drop(X)\n",
    "\n",
    "        X = torch.cat([X, torch.unsqueeze(X_cont,1)], 1)\n",
    "        X = X.reshape(1,X.size(1),X.size(0))\n",
    "        out = self.tcn1(X)\n",
    "        out = self.tcn2(out)\n",
    "        out = self.layers(out.view(out.size(2),out.size(1)))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8649dcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCNwithEmbeddings(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(13, 7)\n",
       "    (1): Embedding(32, 16)\n",
       "    (2): Embedding(8, 4)\n",
       "    (3): Embedding(11, 6)\n",
       "    (4): Embedding(51, 26)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0, inplace=False)\n",
       "  (tcn1): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(60, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (conv2): Conv1d(112, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(60, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0, inplace=False)\n",
       "          (4): Conv1d(112, 112, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(60, 112, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tcn2): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(112, 96, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (conv2): Conv1d(96, 96, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(112, 96, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0, inplace=False)\n",
       "          (4): Conv1d(96, 96, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(112, 96, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=96, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiating TCN model with 2 TCN layers, 2 Fully connected layers and an output layer \n",
    "# having 112,96,64 & 32 nodes respectively\n",
    "\n",
    "TCNmodel = TCNwithEmbeddings(embedding_dim[1:], 1, 1, ker_size=2,dense_layers=[64,32], num_channels=[1], dp=0)\n",
    "TCNmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0165a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function (for optional use)\n",
    "def smape(x,y):\n",
    "    return 100*torch.mean(2*torch.abs(x-y)/(torch.abs(x)+torch.abs(y)))\n",
    "\n",
    "optim = torch.optim.Adam(TCNmodel.parameters(), lr = 0.01)\n",
    "lossfn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450725e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50a4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, sets, model, X_train, y_train, lossfn, optimizer):\n",
    "    ep_count = 0\n",
    "    for i in range(sets//2):\n",
    "        for j in range(2):\n",
    "            X_tr, y_tr, X_val, y_val = split_data(X_train, y_train, val_year=i, half_yearly=j, randomly=False)\n",
    "            losses = []\n",
    "            for k in range(epochs):\n",
    "                k+=1\n",
    "                y_pred = TCNmodel(torch.from_numpy(X_tr[:,1:]), torch.from_numpy(X_tr[:,0]))\n",
    "                y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n",
    "                loss = lossfn(y_pred,torch.tensor(y_tr))\n",
    "                losses.append(loss)\n",
    "                print(\"Epoch number {} of validation year 20{} and half {} has MSE loss {}\".format(k,13+i,j,loss.item()))\n",
    "                ep_count+=1\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if k%10 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        yhat_val = TCNmodel(torch.from_numpy(X_val[:,1:]), torch.from_numpy(X_val[:,0]))\n",
    "                        y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n",
    "                        val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n",
    "                    print(\"Validation loss at epoch {} of year 20{} half {} is {:.4f}\".format(k,13+i,j,val_loss.item()))\n",
    "    \n",
    "    print(\"Total number of epochs is\", ep_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833d5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9790/354890347.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = lossfn(y_pred,torch.tensor(y_tr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1 of validation year 2013 and half 0 has MSE loss 3435.500244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9790/354890347.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 2 of validation year 2013 and half 0 has MSE loss 3427.930908203125\n",
      "Epoch number 3 of validation year 2013 and half 0 has MSE loss 3366.068115234375\n",
      "Epoch number 4 of validation year 2013 and half 0 has MSE loss 3056.044189453125\n",
      "Epoch number 5 of validation year 2013 and half 0 has MSE loss 3032.1240234375\n",
      "Epoch number 6 of validation year 2013 and half 0 has MSE loss 2656.79345703125\n",
      "Epoch number 7 of validation year 2013 and half 0 has MSE loss 2698.4970703125\n",
      "Epoch number 8 of validation year 2013 and half 0 has MSE loss 2637.587158203125\n",
      "Epoch number 9 of validation year 2013 and half 0 has MSE loss 2557.9013671875\n",
      "Epoch number 10 of validation year 2013 and half 0 has MSE loss 2659.690185546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9790/354890347.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 10 of year 2013 half 0 is 44.3446\n",
      "Epoch number 11 of validation year 2013 and half 0 has MSE loss 2548.502685546875\n",
      "Epoch number 12 of validation year 2013 and half 0 has MSE loss 2541.233154296875\n",
      "Epoch number 13 of validation year 2013 and half 0 has MSE loss 2560.31884765625\n",
      "Epoch number 14 of validation year 2013 and half 0 has MSE loss 2520.39111328125\n",
      "Epoch number 15 of validation year 2013 and half 0 has MSE loss 2474.1640625\n",
      "Epoch number 16 of validation year 2013 and half 0 has MSE loss 2507.276123046875\n",
      "Epoch number 17 of validation year 2013 and half 0 has MSE loss 2473.999755859375\n",
      "Epoch number 18 of validation year 2013 and half 0 has MSE loss 2443.62451171875\n",
      "Epoch number 19 of validation year 2013 and half 0 has MSE loss 2453.67431640625\n",
      "Epoch number 20 of validation year 2013 and half 0 has MSE loss 2444.47509765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9790/354890347.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 20 of year 2013 half 0 is 42.6721\n",
      "Epoch number 21 of validation year 2013 and half 0 has MSE loss 2412.85205078125\n",
      "Epoch number 22 of validation year 2013 and half 0 has MSE loss 2402.53955078125\n",
      "Epoch number 23 of validation year 2013 and half 0 has MSE loss 2395.778076171875\n",
      "Epoch number 24 of validation year 2013 and half 0 has MSE loss 2368.671875\n",
      "Epoch number 25 of validation year 2013 and half 0 has MSE loss 2349.00048828125\n",
      "Epoch number 26 of validation year 2013 and half 0 has MSE loss 2337.1728515625\n",
      "Epoch number 27 of validation year 2013 and half 0 has MSE loss 2316.575927734375\n",
      "Epoch number 28 of validation year 2013 and half 0 has MSE loss 2289.305908203125\n",
      "Epoch number 29 of validation year 2013 and half 0 has MSE loss 2267.835205078125\n",
      "Epoch number 30 of validation year 2013 and half 0 has MSE loss 2245.180419921875\n",
      "Validation loss at epoch 30 of year 2013 half 0 is 40.8194\n",
      "Epoch number 31 of validation year 2013 and half 0 has MSE loss 2211.713623046875\n",
      "Epoch number 32 of validation year 2013 and half 0 has MSE loss 2180.09228515625\n",
      "Epoch number 33 of validation year 2013 and half 0 has MSE loss 2148.748046875\n",
      "Epoch number 34 of validation year 2013 and half 0 has MSE loss 2108.31103515625\n",
      "Epoch number 35 of validation year 2013 and half 0 has MSE loss 2064.582763671875\n",
      "Epoch number 36 of validation year 2013 and half 0 has MSE loss 2022.6063232421875\n",
      "Epoch number 37 of validation year 2013 and half 0 has MSE loss 1970.5634765625\n",
      "Epoch number 38 of validation year 2013 and half 0 has MSE loss 1916.6060791015625\n",
      "Epoch number 39 of validation year 2013 and half 0 has MSE loss 1861.08251953125\n",
      "Epoch number 40 of validation year 2013 and half 0 has MSE loss 1796.487060546875\n",
      "Validation loss at epoch 40 of year 2013 half 0 is 35.4710\n",
      "Epoch number 41 of validation year 2013 and half 0 has MSE loss 1732.5845947265625\n",
      "Epoch number 42 of validation year 2013 and half 0 has MSE loss 1661.6490478515625\n",
      "Epoch number 43 of validation year 2013 and half 0 has MSE loss 1587.3603515625\n",
      "Epoch number 44 of validation year 2013 and half 0 has MSE loss 1511.6766357421875\n",
      "Epoch number 45 of validation year 2013 and half 0 has MSE loss 1428.745361328125\n",
      "Epoch number 46 of validation year 2013 and half 0 has MSE loss 1348.1888427734375\n",
      "Epoch number 47 of validation year 2013 and half 0 has MSE loss 1261.65380859375\n",
      "Epoch number 48 of validation year 2013 and half 0 has MSE loss 1179.0740966796875\n",
      "Epoch number 49 of validation year 2013 and half 0 has MSE loss 1093.0845947265625\n",
      "Epoch number 50 of validation year 2013 and half 0 has MSE loss 1013.4667358398438\n",
      "Validation loss at epoch 50 of year 2013 half 0 is 24.7327\n",
      "Epoch number 1 of validation year 2013 and half 1 has MSE loss 944.311279296875\n",
      "Epoch number 2 of validation year 2013 and half 1 has MSE loss 871.53857421875\n",
      "Epoch number 3 of validation year 2013 and half 1 has MSE loss 807.2000122070312\n",
      "Epoch number 4 of validation year 2013 and half 1 has MSE loss 751.4197998046875\n",
      "Epoch number 5 of validation year 2013 and half 1 has MSE loss 710.279296875\n",
      "Epoch number 6 of validation year 2013 and half 1 has MSE loss 679.7179565429688\n",
      "Epoch number 7 of validation year 2013 and half 1 has MSE loss 664.1981811523438\n",
      "Epoch number 8 of validation year 2013 and half 1 has MSE loss 663.0052490234375\n",
      "Epoch number 9 of validation year 2013 and half 1 has MSE loss 671.2091064453125\n",
      "Epoch number 10 of validation year 2013 and half 1 has MSE loss 688.5722045898438\n",
      "Validation loss at epoch 10 of year 2013 half 1 is 26.4421\n",
      "Epoch number 11 of validation year 2013 and half 1 has MSE loss 710.549072265625\n",
      "Epoch number 12 of validation year 2013 and half 1 has MSE loss 730.2244873046875\n",
      "Epoch number 13 of validation year 2013 and half 1 has MSE loss 745.4224853515625\n",
      "Epoch number 14 of validation year 2013 and half 1 has MSE loss 754.3746948242188\n",
      "Epoch number 15 of validation year 2013 and half 1 has MSE loss 754.9337158203125\n",
      "Epoch number 16 of validation year 2013 and half 1 has MSE loss 747.6006469726562\n",
      "Epoch number 17 of validation year 2013 and half 1 has MSE loss 735.0916137695312\n",
      "Epoch number 18 of validation year 2013 and half 1 has MSE loss 720.03076171875\n",
      "Epoch number 19 of validation year 2013 and half 1 has MSE loss 703.8630981445312\n",
      "Epoch number 20 of validation year 2013 and half 1 has MSE loss 688.074951171875\n",
      "Validation loss at epoch 20 of year 2013 half 1 is 25.1906\n",
      "Epoch number 21 of validation year 2013 and half 1 has MSE loss 674.9380493164062\n",
      "Epoch number 22 of validation year 2013 and half 1 has MSE loss 665.0254516601562\n",
      "Epoch number 23 of validation year 2013 and half 1 has MSE loss 657.7525634765625\n",
      "Epoch number 24 of validation year 2013 and half 1 has MSE loss 653.5814208984375\n",
      "Epoch number 25 of validation year 2013 and half 1 has MSE loss 651.9666748046875\n",
      "Epoch number 26 of validation year 2013 and half 1 has MSE loss 651.7747802734375\n",
      "Epoch number 27 of validation year 2013 and half 1 has MSE loss 652.9328002929688\n",
      "Epoch number 28 of validation year 2013 and half 1 has MSE loss 654.45654296875\n",
      "Epoch number 29 of validation year 2013 and half 1 has MSE loss 655.8807373046875\n",
      "Epoch number 30 of validation year 2013 and half 1 has MSE loss 657.040771484375\n",
      "Validation loss at epoch 30 of year 2013 half 1 is 22.0500\n",
      "Epoch number 31 of validation year 2013 and half 1 has MSE loss 657.359619140625\n",
      "Epoch number 32 of validation year 2013 and half 1 has MSE loss 652.7097778320312\n",
      "Epoch number 33 of validation year 2013 and half 1 has MSE loss 648.2578125\n",
      "Epoch number 34 of validation year 2013 and half 1 has MSE loss 646.348876953125\n",
      "Epoch number 35 of validation year 2013 and half 1 has MSE loss 643.7305297851562\n",
      "Epoch number 36 of validation year 2013 and half 1 has MSE loss 640.7396240234375\n",
      "Epoch number 37 of validation year 2013 and half 1 has MSE loss 637.890380859375\n",
      "Epoch number 38 of validation year 2013 and half 1 has MSE loss 635.1380615234375\n",
      "Epoch number 39 of validation year 2013 and half 1 has MSE loss 632.80419921875\n",
      "Epoch number 40 of validation year 2013 and half 1 has MSE loss 631.0966186523438\n",
      "Validation loss at epoch 40 of year 2013 half 1 is 22.5137\n",
      "Epoch number 41 of validation year 2013 and half 1 has MSE loss 629.2779541015625\n",
      "Epoch number 42 of validation year 2013 and half 1 has MSE loss 628.11083984375\n",
      "Epoch number 43 of validation year 2013 and half 1 has MSE loss 627.281005859375\n",
      "Epoch number 44 of validation year 2013 and half 1 has MSE loss 626.73046875\n",
      "Epoch number 45 of validation year 2013 and half 1 has MSE loss 626.279296875\n",
      "Epoch number 46 of validation year 2013 and half 1 has MSE loss 624.953369140625\n",
      "Epoch number 47 of validation year 2013 and half 1 has MSE loss 615.7163696289062\n",
      "Epoch number 48 of validation year 2013 and half 1 has MSE loss 615.7390747070312\n",
      "Epoch number 49 of validation year 2013 and half 1 has MSE loss 615.6327514648438\n",
      "Epoch number 50 of validation year 2013 and half 1 has MSE loss 615.367919921875\n",
      "Validation loss at epoch 50 of year 2013 half 1 is 23.2463\n",
      "Epoch number 1 of validation year 2014 and half 0 has MSE loss 610.7786254882812\n",
      "Epoch number 2 of validation year 2014 and half 0 has MSE loss 612.00927734375\n",
      "Epoch number 3 of validation year 2014 and half 0 has MSE loss 609.5106201171875\n",
      "Epoch number 4 of validation year 2014 and half 0 has MSE loss 610.3045654296875\n",
      "Epoch number 5 of validation year 2014 and half 0 has MSE loss 608.7986450195312\n",
      "Epoch number 6 of validation year 2014 and half 0 has MSE loss 606.5989379882812\n",
      "Epoch number 7 of validation year 2014 and half 0 has MSE loss 606.8777465820312\n",
      "Epoch number 8 of validation year 2014 and half 0 has MSE loss 604.431640625\n",
      "Epoch number 9 of validation year 2014 and half 0 has MSE loss 603.6693725585938\n",
      "Epoch number 10 of validation year 2014 and half 0 has MSE loss 603.2625732421875\n",
      "Validation loss at epoch 10 of year 2014 half 0 is 24.1936\n",
      "Epoch number 11 of validation year 2014 and half 0 has MSE loss 601.672607421875\n",
      "Epoch number 12 of validation year 2014 and half 0 has MSE loss 602.0142822265625\n",
      "Epoch number 13 of validation year 2014 and half 0 has MSE loss 601.2276611328125\n",
      "Epoch number 14 of validation year 2014 and half 0 has MSE loss 601.1009521484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 15 of validation year 2014 and half 0 has MSE loss 601.2650146484375\n",
      "Epoch number 16 of validation year 2014 and half 0 has MSE loss 600.5956420898438\n",
      "Epoch number 17 of validation year 2014 and half 0 has MSE loss 600.885986328125\n",
      "Epoch number 18 of validation year 2014 and half 0 has MSE loss 600.2864990234375\n",
      "Epoch number 19 of validation year 2014 and half 0 has MSE loss 599.8524169921875\n",
      "Epoch number 20 of validation year 2014 and half 0 has MSE loss 599.6129150390625\n",
      "Validation loss at epoch 20 of year 2014 half 0 is 24.2148\n",
      "Epoch number 21 of validation year 2014 and half 0 has MSE loss 598.67626953125\n",
      "Epoch number 22 of validation year 2014 and half 0 has MSE loss 598.298583984375\n",
      "Epoch number 23 of validation year 2014 and half 0 has MSE loss 597.5454711914062\n",
      "Epoch number 24 of validation year 2014 and half 0 has MSE loss 596.8092651367188\n",
      "Epoch number 25 of validation year 2014 and half 0 has MSE loss 596.3521728515625\n",
      "Epoch number 26 of validation year 2014 and half 0 has MSE loss 595.5593872070312\n",
      "Epoch number 27 of validation year 2014 and half 0 has MSE loss 595.1307373046875\n",
      "Epoch number 28 of validation year 2014 and half 0 has MSE loss 594.6193237304688\n",
      "Epoch number 29 of validation year 2014 and half 0 has MSE loss 594.0679321289062\n",
      "Epoch number 30 of validation year 2014 and half 0 has MSE loss 593.8067626953125\n",
      "Validation loss at epoch 30 of year 2014 half 0 is 24.0530\n",
      "Epoch number 31 of validation year 2014 and half 0 has MSE loss 593.4581909179688\n",
      "Epoch number 32 of validation year 2014 and half 0 has MSE loss 593.1096801757812\n",
      "Epoch number 33 of validation year 2014 and half 0 has MSE loss 592.9190673828125\n",
      "Epoch number 34 of validation year 2014 and half 0 has MSE loss 592.5037231445312\n",
      "Epoch number 35 of validation year 2014 and half 0 has MSE loss 592.3948364257812\n",
      "Epoch number 36 of validation year 2014 and half 0 has MSE loss 592.1725463867188\n",
      "Epoch number 37 of validation year 2014 and half 0 has MSE loss 591.88720703125\n",
      "Epoch number 38 of validation year 2014 and half 0 has MSE loss 591.7989501953125\n",
      "Epoch number 39 of validation year 2014 and half 0 has MSE loss 591.5559692382812\n",
      "Epoch number 40 of validation year 2014 and half 0 has MSE loss 591.2410278320312\n",
      "Validation loss at epoch 40 of year 2014 half 0 is 24.0307\n",
      "Epoch number 41 of validation year 2014 and half 0 has MSE loss 590.9146118164062\n",
      "Epoch number 42 of validation year 2014 and half 0 has MSE loss 590.313232421875\n",
      "Epoch number 43 of validation year 2014 and half 0 has MSE loss 587.74755859375\n",
      "Epoch number 44 of validation year 2014 and half 0 has MSE loss 580.9054565429688\n",
      "Epoch number 45 of validation year 2014 and half 0 has MSE loss 576.283935546875\n",
      "Epoch number 46 of validation year 2014 and half 0 has MSE loss 576.1575317382812\n",
      "Epoch number 47 of validation year 2014 and half 0 has MSE loss 577.4364624023438\n",
      "Epoch number 48 of validation year 2014 and half 0 has MSE loss 577.0927734375\n",
      "Epoch number 49 of validation year 2014 and half 0 has MSE loss 577.7362060546875\n",
      "Epoch number 50 of validation year 2014 and half 0 has MSE loss 577.7764892578125\n",
      "Validation loss at epoch 50 of year 2014 half 0 is 23.7679\n",
      "Epoch number 1 of validation year 2014 and half 1 has MSE loss 582.065673828125\n",
      "Epoch number 2 of validation year 2014 and half 1 has MSE loss 581.6209716796875\n",
      "Epoch number 3 of validation year 2014 and half 1 has MSE loss 580.7993774414062\n",
      "Epoch number 4 of validation year 2014 and half 1 has MSE loss 579.6815795898438\n",
      "Epoch number 5 of validation year 2014 and half 1 has MSE loss 579.051025390625\n",
      "Epoch number 6 of validation year 2014 and half 1 has MSE loss 578.9501342773438\n",
      "Epoch number 7 of validation year 2014 and half 1 has MSE loss 578.7067260742188\n",
      "Epoch number 8 of validation year 2014 and half 1 has MSE loss 578.3465576171875\n",
      "Epoch number 9 of validation year 2014 and half 1 has MSE loss 578.2280883789062\n",
      "Epoch number 10 of validation year 2014 and half 1 has MSE loss 578.2998046875\n",
      "Validation loss at epoch 10 of year 2014 half 1 is 22.9867\n",
      "Epoch number 11 of validation year 2014 and half 1 has MSE loss 578.233154296875\n",
      "Epoch number 12 of validation year 2014 and half 1 has MSE loss 578.0167236328125\n",
      "Epoch number 13 of validation year 2014 and half 1 has MSE loss 577.88232421875\n",
      "Epoch number 14 of validation year 2014 and half 1 has MSE loss 577.8239135742188\n",
      "Epoch number 15 of validation year 2014 and half 1 has MSE loss 577.6566162109375\n",
      "Epoch number 16 of validation year 2014 and half 1 has MSE loss 577.3856201171875\n",
      "Epoch number 17 of validation year 2014 and half 1 has MSE loss 577.1718139648438\n",
      "Epoch number 18 of validation year 2014 and half 1 has MSE loss 577.0595703125\n",
      "Epoch number 19 of validation year 2014 and half 1 has MSE loss 576.9254150390625\n",
      "Epoch number 20 of validation year 2014 and half 1 has MSE loss 576.7052001953125\n",
      "Validation loss at epoch 20 of year 2014 half 1 is 22.9845\n",
      "Epoch number 21 of validation year 2014 and half 1 has MSE loss 576.4957275390625\n",
      "Epoch number 22 of validation year 2014 and half 1 has MSE loss 576.3642578125\n",
      "Epoch number 23 of validation year 2014 and half 1 has MSE loss 576.2538452148438\n",
      "Epoch number 24 of validation year 2014 and half 1 has MSE loss 576.0996704101562\n",
      "Epoch number 25 of validation year 2014 and half 1 has MSE loss 575.9251098632812\n",
      "Epoch number 26 of validation year 2014 and half 1 has MSE loss 575.7896118164062\n",
      "Epoch number 27 of validation year 2014 and half 1 has MSE loss 575.6885986328125\n",
      "Epoch number 28 of validation year 2014 and half 1 has MSE loss 575.5739135742188\n",
      "Epoch number 29 of validation year 2014 and half 1 has MSE loss 575.426513671875\n",
      "Epoch number 30 of validation year 2014 and half 1 has MSE loss 575.27490234375\n",
      "Validation loss at epoch 30 of year 2014 half 1 is 22.9627\n",
      "Epoch number 31 of validation year 2014 and half 1 has MSE loss 575.1464233398438\n",
      "Epoch number 32 of validation year 2014 and half 1 has MSE loss 575.0316162109375\n",
      "Epoch number 33 of validation year 2014 and half 1 has MSE loss 574.9058227539062\n",
      "Epoch number 34 of validation year 2014 and half 1 has MSE loss 574.7637939453125\n",
      "Epoch number 35 of validation year 2014 and half 1 has MSE loss 574.6190795898438\n",
      "Epoch number 36 of validation year 2014 and half 1 has MSE loss 574.4868774414062\n",
      "Epoch number 37 of validation year 2014 and half 1 has MSE loss 574.367919921875\n",
      "Epoch number 38 of validation year 2014 and half 1 has MSE loss 574.1357421875\n",
      "Epoch number 39 of validation year 2014 and half 1 has MSE loss 575.3114624023438\n",
      "Epoch number 40 of validation year 2014 and half 1 has MSE loss 573.6046752929688\n",
      "Validation loss at epoch 40 of year 2014 half 1 is 22.9844\n",
      "Epoch number 41 of validation year 2014 and half 1 has MSE loss 573.4337158203125\n",
      "Epoch number 42 of validation year 2014 and half 1 has MSE loss 573.0732421875\n",
      "Epoch number 43 of validation year 2014 and half 1 has MSE loss 572.7852172851562\n",
      "Epoch number 44 of validation year 2014 and half 1 has MSE loss 572.806884765625\n",
      "Epoch number 45 of validation year 2014 and half 1 has MSE loss 572.65087890625\n",
      "Epoch number 46 of validation year 2014 and half 1 has MSE loss 572.3966064453125\n",
      "Epoch number 47 of validation year 2014 and half 1 has MSE loss 572.3184204101562\n",
      "Epoch number 48 of validation year 2014 and half 1 has MSE loss 572.2359008789062\n",
      "Epoch number 49 of validation year 2014 and half 1 has MSE loss 572.0335693359375\n",
      "Epoch number 50 of validation year 2014 and half 1 has MSE loss 571.2528686523438\n",
      "Validation loss at epoch 50 of year 2014 half 1 is 22.9805\n",
      "Epoch number 1 of validation year 2015 and half 0 has MSE loss 560.4100341796875\n",
      "Epoch number 2 of validation year 2015 and half 0 has MSE loss 562.2194213867188\n",
      "Epoch number 3 of validation year 2015 and half 0 has MSE loss 563.5106201171875\n",
      "Epoch number 4 of validation year 2015 and half 0 has MSE loss 564.184326171875\n",
      "Epoch number 5 of validation year 2015 and half 0 has MSE loss 560.5586547851562\n",
      "Epoch number 6 of validation year 2015 and half 0 has MSE loss 558.516357421875\n",
      "Epoch number 7 of validation year 2015 and half 0 has MSE loss 559.1036987304688\n",
      "Epoch number 8 of validation year 2015 and half 0 has MSE loss 560.0796508789062\n",
      "Epoch number 9 of validation year 2015 and half 0 has MSE loss 559.747314453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10 of validation year 2015 and half 0 has MSE loss 557.7366333007812\n",
      "Validation loss at epoch 10 of year 2015 half 0 is 25.0553\n",
      "Epoch number 11 of validation year 2015 and half 0 has MSE loss 557.4647827148438\n",
      "Epoch number 12 of validation year 2015 and half 0 has MSE loss 558.4695434570312\n",
      "Epoch number 13 of validation year 2015 and half 0 has MSE loss 557.883544921875\n",
      "Epoch number 14 of validation year 2015 and half 0 has MSE loss 556.7520141601562\n",
      "Epoch number 15 of validation year 2015 and half 0 has MSE loss 556.5944213867188\n",
      "Epoch number 16 of validation year 2015 and half 0 has MSE loss 557.184326171875\n",
      "Epoch number 17 of validation year 2015 and half 0 has MSE loss 557.2127685546875\n",
      "Epoch number 18 of validation year 2015 and half 0 has MSE loss 556.3094482421875\n",
      "Epoch number 19 of validation year 2015 and half 0 has MSE loss 555.9057006835938\n",
      "Epoch number 20 of validation year 2015 and half 0 has MSE loss 556.2853393554688\n",
      "Validation loss at epoch 20 of year 2015 half 0 is 25.0170\n",
      "Epoch number 21 of validation year 2015 and half 0 has MSE loss 556.2289428710938\n",
      "Epoch number 22 of validation year 2015 and half 0 has MSE loss 555.7203979492188\n",
      "Epoch number 23 of validation year 2015 and half 0 has MSE loss 555.3690185546875\n",
      "Epoch number 24 of validation year 2015 and half 0 has MSE loss 555.4805297851562\n",
      "Epoch number 25 of validation year 2015 and half 0 has MSE loss 555.6162719726562\n",
      "Epoch number 26 of validation year 2015 and half 0 has MSE loss 555.3848876953125\n",
      "Epoch number 27 of validation year 2015 and half 0 has MSE loss 555.0204467773438\n",
      "Epoch number 28 of validation year 2015 and half 0 has MSE loss 554.8422241210938\n",
      "Epoch number 29 of validation year 2015 and half 0 has MSE loss 554.921630859375\n",
      "Epoch number 30 of validation year 2015 and half 0 has MSE loss 554.9560546875\n",
      "Validation loss at epoch 30 of year 2015 half 0 is 25.0402\n",
      "Epoch number 31 of validation year 2015 and half 0 has MSE loss 554.703857421875\n",
      "Epoch number 32 of validation year 2015 and half 0 has MSE loss 554.4308471679688\n",
      "Epoch number 33 of validation year 2015 and half 0 has MSE loss 553.769775390625\n",
      "Epoch number 34 of validation year 2015 and half 0 has MSE loss 552.0308837890625\n",
      "Epoch number 35 of validation year 2015 and half 0 has MSE loss 549.7948608398438\n",
      "Epoch number 36 of validation year 2015 and half 0 has MSE loss 543.6070556640625\n",
      "Epoch number 37 of validation year 2015 and half 0 has MSE loss 543.4011840820312\n",
      "Epoch number 38 of validation year 2015 and half 0 has MSE loss 543.1792602539062\n",
      "Epoch number 39 of validation year 2015 and half 0 has MSE loss 543.6305541992188\n",
      "Epoch number 40 of validation year 2015 and half 0 has MSE loss 543.7395629882812\n",
      "Validation loss at epoch 40 of year 2015 half 0 is 24.8500\n",
      "Epoch number 41 of validation year 2015 and half 0 has MSE loss 543.5429077148438\n",
      "Epoch number 42 of validation year 2015 and half 0 has MSE loss 543.1032104492188\n",
      "Epoch number 43 of validation year 2015 and half 0 has MSE loss 542.7371215820312\n",
      "Epoch number 44 of validation year 2015 and half 0 has MSE loss 542.5941772460938\n",
      "Epoch number 45 of validation year 2015 and half 0 has MSE loss 542.4664306640625\n",
      "Epoch number 46 of validation year 2015 and half 0 has MSE loss 542.353515625\n",
      "Epoch number 47 of validation year 2015 and half 0 has MSE loss 542.1837768554688\n",
      "Epoch number 48 of validation year 2015 and half 0 has MSE loss 542.0551147460938\n",
      "Epoch number 49 of validation year 2015 and half 0 has MSE loss 541.93603515625\n",
      "Epoch number 50 of validation year 2015 and half 0 has MSE loss 541.7962646484375\n",
      "Validation loss at epoch 50 of year 2015 half 0 is 24.8324\n",
      "Epoch number 1 of validation year 2015 and half 1 has MSE loss 549.6375732421875\n",
      "Epoch number 2 of validation year 2015 and half 1 has MSE loss 550.16748046875\n",
      "Epoch number 3 of validation year 2015 and half 1 has MSE loss 551.9583740234375\n",
      "Epoch number 4 of validation year 2015 and half 1 has MSE loss 554.4097290039062\n",
      "Epoch number 5 of validation year 2015 and half 1 has MSE loss 560.1524658203125\n",
      "Epoch number 6 of validation year 2015 and half 1 has MSE loss 556.6336059570312\n",
      "Epoch number 7 of validation year 2015 and half 1 has MSE loss 552.9607543945312\n",
      "Epoch number 8 of validation year 2015 and half 1 has MSE loss 548.8271484375\n",
      "Epoch number 9 of validation year 2015 and half 1 has MSE loss 551.4017944335938\n",
      "Epoch number 10 of validation year 2015 and half 1 has MSE loss 554.6149291992188\n",
      "Validation loss at epoch 10 of year 2015 half 1 is 23.5304\n",
      "Epoch number 11 of validation year 2015 and half 1 has MSE loss 549.581298828125\n",
      "Epoch number 12 of validation year 2015 and half 1 has MSE loss 549.343017578125\n",
      "Epoch number 13 of validation year 2015 and half 1 has MSE loss 552.3732299804688\n",
      "Epoch number 14 of validation year 2015 and half 1 has MSE loss 549.2086791992188\n",
      "Epoch number 15 of validation year 2015 and half 1 has MSE loss 548.69189453125\n",
      "Epoch number 16 of validation year 2015 and half 1 has MSE loss 550.7091064453125\n",
      "Epoch number 17 of validation year 2015 and half 1 has MSE loss 548.5225219726562\n",
      "Epoch number 18 of validation year 2015 and half 1 has MSE loss 548.376708984375\n",
      "Epoch number 19 of validation year 2015 and half 1 has MSE loss 549.6785278320312\n",
      "Epoch number 20 of validation year 2015 and half 1 has MSE loss 547.95947265625\n",
      "Validation loss at epoch 20 of year 2015 half 1 is 23.5188\n",
      "Epoch number 21 of validation year 2015 and half 1 has MSE loss 548.1491088867188\n",
      "Epoch number 22 of validation year 2015 and half 1 has MSE loss 548.9707641601562\n",
      "Epoch number 23 of validation year 2015 and half 1 has MSE loss 547.54541015625\n",
      "Epoch number 24 of validation year 2015 and half 1 has MSE loss 547.9735717773438\n",
      "Epoch number 25 of validation year 2015 and half 1 has MSE loss 548.440673828125\n",
      "Epoch number 26 of validation year 2015 and half 1 has MSE loss 547.238525390625\n",
      "Epoch number 27 of validation year 2015 and half 1 has MSE loss 547.8043212890625\n",
      "Epoch number 28 of validation year 2015 and half 1 has MSE loss 548.014892578125\n",
      "Epoch number 29 of validation year 2015 and half 1 has MSE loss 546.9886474609375\n",
      "Epoch number 30 of validation year 2015 and half 1 has MSE loss 547.5946044921875\n",
      "Validation loss at epoch 30 of year 2015 half 1 is 23.5340\n",
      "Epoch number 31 of validation year 2015 and half 1 has MSE loss 547.64599609375\n",
      "Epoch number 32 of validation year 2015 and half 1 has MSE loss 546.7649536132812\n",
      "Epoch number 33 of validation year 2015 and half 1 has MSE loss 547.3591918945312\n",
      "Epoch number 34 of validation year 2015 and half 1 has MSE loss 547.336669921875\n",
      "Epoch number 35 of validation year 2015 and half 1 has MSE loss 546.5648803710938\n",
      "Epoch number 36 of validation year 2015 and half 1 has MSE loss 547.1060791015625\n",
      "Epoch number 37 of validation year 2015 and half 1 has MSE loss 547.0670166015625\n",
      "Epoch number 38 of validation year 2015 and half 1 has MSE loss 546.3807983398438\n",
      "Epoch number 39 of validation year 2015 and half 1 has MSE loss 546.848876953125\n",
      "Epoch number 40 of validation year 2015 and half 1 has MSE loss 546.8240966796875\n",
      "Validation loss at epoch 40 of year 2015 half 1 is 23.5317\n",
      "Epoch number 41 of validation year 2015 and half 1 has MSE loss 546.1998291015625\n",
      "Epoch number 42 of validation year 2015 and half 1 has MSE loss 546.5850830078125\n",
      "Epoch number 43 of validation year 2015 and half 1 has MSE loss 546.594482421875\n",
      "Epoch number 44 of validation year 2015 and half 1 has MSE loss 546.0244750976562\n",
      "Epoch number 45 of validation year 2015 and half 1 has MSE loss 546.3206176757812\n",
      "Epoch number 46 of validation year 2015 and half 1 has MSE loss 546.3789672851562\n",
      "Epoch number 47 of validation year 2015 and half 1 has MSE loss 545.8623046875\n",
      "Epoch number 48 of validation year 2015 and half 1 has MSE loss 546.05517578125\n",
      "Epoch number 49 of validation year 2015 and half 1 has MSE loss 546.163818359375\n",
      "Epoch number 50 of validation year 2015 and half 1 has MSE loss 545.7113647460938\n",
      "Validation loss at epoch 50 of year 2015 half 1 is 23.5430\n",
      "Epoch number 1 of validation year 2016 and half 0 has MSE loss 523.525634765625\n",
      "Epoch number 2 of validation year 2016 and half 0 has MSE loss 525.8985595703125\n",
      "Epoch number 3 of validation year 2016 and half 0 has MSE loss 521.5581665039062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 4 of validation year 2016 and half 0 has MSE loss 523.5337524414062\n",
      "Epoch number 5 of validation year 2016 and half 0 has MSE loss 524.0191650390625\n",
      "Epoch number 6 of validation year 2016 and half 0 has MSE loss 520.2859497070312\n",
      "Epoch number 7 of validation year 2016 and half 0 has MSE loss 523.6299438476562\n",
      "Epoch number 8 of validation year 2016 and half 0 has MSE loss 522.2738647460938\n",
      "Epoch number 9 of validation year 2016 and half 0 has MSE loss 520.477783203125\n",
      "Epoch number 10 of validation year 2016 and half 0 has MSE loss 523.5716552734375\n",
      "Validation loss at epoch 10 of year 2016 half 0 is 28.6523\n",
      "Epoch number 11 of validation year 2016 and half 0 has MSE loss 520.6221923828125\n",
      "Epoch number 12 of validation year 2016 and half 0 has MSE loss 521.19970703125\n",
      "Epoch number 13 of validation year 2016 and half 0 has MSE loss 521.6058349609375\n",
      "Epoch number 14 of validation year 2016 and half 0 has MSE loss 519.6415405273438\n",
      "Epoch number 15 of validation year 2016 and half 0 has MSE loss 521.04541015625\n",
      "Epoch number 16 of validation year 2016 and half 0 has MSE loss 519.5531616210938\n",
      "Epoch number 17 of validation year 2016 and half 0 has MSE loss 520.1011352539062\n",
      "Epoch number 18 of validation year 2016 and half 0 has MSE loss 519.9924926757812\n",
      "Epoch number 19 of validation year 2016 and half 0 has MSE loss 519.2485961914062\n",
      "Epoch number 20 of validation year 2016 and half 0 has MSE loss 520.0901489257812\n",
      "Validation loss at epoch 20 of year 2016 half 0 is 28.3586\n",
      "Epoch number 21 of validation year 2016 and half 0 has MSE loss 519.0094604492188\n",
      "Epoch number 22 of validation year 2016 and half 0 has MSE loss 519.5291137695312\n",
      "Epoch number 23 of validation year 2016 and half 0 has MSE loss 519.1913452148438\n",
      "Epoch number 24 of validation year 2016 and half 0 has MSE loss 518.982666015625\n",
      "Epoch number 25 of validation year 2016 and half 0 has MSE loss 519.30712890625\n",
      "Epoch number 26 of validation year 2016 and half 0 has MSE loss 518.7335205078125\n",
      "Epoch number 27 of validation year 2016 and half 0 has MSE loss 519.1534423828125\n",
      "Epoch number 28 of validation year 2016 and half 0 has MSE loss 518.7621459960938\n",
      "Epoch number 29 of validation year 2016 and half 0 has MSE loss 518.7576904296875\n",
      "Epoch number 30 of validation year 2016 and half 0 has MSE loss 518.8169555664062\n",
      "Validation loss at epoch 30 of year 2016 half 0 is 28.2637\n",
      "Epoch number 31 of validation year 2016 and half 0 has MSE loss 518.4706420898438\n",
      "Epoch number 32 of validation year 2016 and half 0 has MSE loss 518.6936645507812\n",
      "Epoch number 33 of validation year 2016 and half 0 has MSE loss 518.4165649414062\n",
      "Epoch number 34 of validation year 2016 and half 0 has MSE loss 518.4739990234375\n",
      "Epoch number 35 of validation year 2016 and half 0 has MSE loss 518.4588623046875\n",
      "Epoch number 36 of validation year 2016 and half 0 has MSE loss 518.2872924804688\n",
      "Epoch number 37 of validation year 2016 and half 0 has MSE loss 518.4224243164062\n",
      "Epoch number 38 of validation year 2016 and half 0 has MSE loss 518.2286987304688\n",
      "Epoch number 39 of validation year 2016 and half 0 has MSE loss 518.2565307617188\n",
      "Epoch number 40 of validation year 2016 and half 0 has MSE loss 518.2296752929688\n",
      "Validation loss at epoch 40 of year 2016 half 0 is 28.3046\n",
      "Epoch number 41 of validation year 2016 and half 0 has MSE loss 518.1036376953125\n",
      "Epoch number 42 of validation year 2016 and half 0 has MSE loss 518.1626586914062\n",
      "Epoch number 43 of validation year 2016 and half 0 has MSE loss 518.0386962890625\n",
      "Epoch number 44 of validation year 2016 and half 0 has MSE loss 518.0316772460938\n",
      "Epoch number 45 of validation year 2016 and half 0 has MSE loss 518.01416015625\n",
      "Epoch number 46 of validation year 2016 and half 0 has MSE loss 517.9194946289062\n",
      "Epoch number 47 of validation year 2016 and half 0 has MSE loss 517.9481811523438\n",
      "Epoch number 48 of validation year 2016 and half 0 has MSE loss 517.8738403320312\n",
      "Epoch number 49 of validation year 2016 and half 0 has MSE loss 517.8403930664062\n",
      "Epoch number 50 of validation year 2016 and half 0 has MSE loss 517.8402709960938\n",
      "Validation loss at epoch 50 of year 2016 half 0 is 28.2694\n",
      "Epoch number 1 of validation year 2016 and half 1 has MSE loss 561.1905517578125\n",
      "Epoch number 2 of validation year 2016 and half 1 has MSE loss 556.5278930664062\n",
      "Epoch number 3 of validation year 2016 and half 1 has MSE loss 551.007568359375\n",
      "Epoch number 4 of validation year 2016 and half 1 has MSE loss 550.2254638671875\n",
      "Epoch number 5 of validation year 2016 and half 1 has MSE loss 550.5231323242188\n",
      "Epoch number 6 of validation year 2016 and half 1 has MSE loss 548.4076538085938\n",
      "Epoch number 7 of validation year 2016 and half 1 has MSE loss 546.0567626953125\n",
      "Epoch number 8 of validation year 2016 and half 1 has MSE loss 544.8833618164062\n",
      "Epoch number 9 of validation year 2016 and half 1 has MSE loss 543.8582153320312\n",
      "Epoch number 10 of validation year 2016 and half 1 has MSE loss 543.5841674804688\n",
      "Validation loss at epoch 10 of year 2016 half 1 is 24.9117\n",
      "Epoch number 11 of validation year 2016 and half 1 has MSE loss 544.1863403320312\n",
      "Epoch number 12 of validation year 2016 and half 1 has MSE loss 542.9224853515625\n",
      "Epoch number 13 of validation year 2016 and half 1 has MSE loss 541.2149658203125\n",
      "Epoch number 14 of validation year 2016 and half 1 has MSE loss 541.0005493164062\n",
      "Epoch number 15 of validation year 2016 and half 1 has MSE loss 540.4990844726562\n",
      "Epoch number 16 of validation year 2016 and half 1 has MSE loss 539.5234375\n",
      "Epoch number 17 of validation year 2016 and half 1 has MSE loss 539.7655639648438\n",
      "Epoch number 18 of validation year 2016 and half 1 has MSE loss 539.2470092773438\n",
      "Epoch number 19 of validation year 2016 and half 1 has MSE loss 538.2604370117188\n",
      "Epoch number 20 of validation year 2016 and half 1 has MSE loss 538.3207397460938\n",
      "Validation loss at epoch 20 of year 2016 half 1 is 25.0743\n",
      "Epoch number 21 of validation year 2016 and half 1 has MSE loss 537.7982788085938\n",
      "Epoch number 22 of validation year 2016 and half 1 has MSE loss 537.2175903320312\n",
      "Epoch number 23 of validation year 2016 and half 1 has MSE loss 537.501708984375\n",
      "Epoch number 24 of validation year 2016 and half 1 has MSE loss 537.02490234375\n",
      "Epoch number 25 of validation year 2016 and half 1 has MSE loss 536.517578125\n",
      "Epoch number 26 of validation year 2016 and half 1 has MSE loss 536.5458374023438\n",
      "Epoch number 27 of validation year 2016 and half 1 has MSE loss 536.0033569335938\n",
      "Epoch number 28 of validation year 2016 and half 1 has MSE loss 535.7582397460938\n",
      "Epoch number 29 of validation year 2016 and half 1 has MSE loss 535.8565673828125\n",
      "Epoch number 30 of validation year 2016 and half 1 has MSE loss 535.403564453125\n",
      "Validation loss at epoch 30 of year 2016 half 1 is 25.0221\n",
      "Epoch number 31 of validation year 2016 and half 1 has MSE loss 535.1591186523438\n",
      "Epoch number 32 of validation year 2016 and half 1 has MSE loss 535.1344604492188\n",
      "Epoch number 33 of validation year 2016 and half 1 has MSE loss 534.7636108398438\n",
      "Epoch number 34 of validation year 2016 and half 1 has MSE loss 534.6171875\n",
      "Epoch number 35 of validation year 2016 and half 1 has MSE loss 534.5966186523438\n",
      "Epoch number 36 of validation year 2016 and half 1 has MSE loss 534.279296875\n",
      "Epoch number 37 of validation year 2016 and half 1 has MSE loss 534.0967407226562\n",
      "Epoch number 38 of validation year 2016 and half 1 has MSE loss 534.0282592773438\n",
      "Epoch number 39 of validation year 2016 and half 1 has MSE loss 533.7728881835938\n",
      "Epoch number 40 of validation year 2016 and half 1 has MSE loss 533.6214599609375\n",
      "Validation loss at epoch 40 of year 2016 half 1 is 24.9844\n",
      "Epoch number 41 of validation year 2016 and half 1 has MSE loss 533.5551147460938\n",
      "Epoch number 42 of validation year 2016 and half 1 has MSE loss 533.3455200195312\n",
      "Epoch number 43 of validation year 2016 and half 1 has MSE loss 533.1914672851562\n",
      "Epoch number 44 of validation year 2016 and half 1 has MSE loss 533.11572265625\n",
      "Epoch number 45 of validation year 2016 and half 1 has MSE loss 532.9447631835938\n",
      "Epoch number 46 of validation year 2016 and half 1 has MSE loss 532.80029296875\n",
      "Epoch number 47 of validation year 2016 and half 1 has MSE loss 532.7291870117188\n",
      "Epoch number 48 of validation year 2016 and half 1 has MSE loss 532.595458984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 49 of validation year 2016 and half 1 has MSE loss 532.4546508789062\n",
      "Epoch number 50 of validation year 2016 and half 1 has MSE loss 532.3786010742188\n",
      "Validation loss at epoch 50 of year 2016 half 1 is 25.0402\n",
      "Total number of epochs is 400\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "begin = time.time()\n",
    "fit(50, 8, TCNmodel, X, y, lossfn, optim)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7bc8ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training in a Ryzen 5 Hexa core 4600H CPU is 97.96 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken for training in a Ryzen 5 Hexa core 4600H CPU is {:.2f} minutes\".format((end-begin)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6ed21",
   "metadata": {},
   "source": [
    "\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c036cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "#import torch_metrics as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bdd7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(test, y_pred_final):\n",
    "    metrics = {'R2_score': r2_score(test, y_pred_final), 'MAE': mean_absolute_error(test, y_pred_final),\n",
    "               'RMSE': mean_squared_error(test, y_pred_final, squared=False),\n",
    "               'MAPE': mean_absolute_percentage_error(test, y_pred_final)}\n",
    "    adj_R2 = 1-(1-metrics['R2_score'])*(len(test)-1)/(len(test)-6-1)      #num of indep var = 6\n",
    "    metrics['adj_R2'] = adj_R2\n",
    "    print(\"R2 score on test set is\", metrics['R2_score'])\n",
    "    print(\"Mean Absolute Error on test set is\", metrics['MAE'])\n",
    "    print(\"Root Mean Square error on test set is\", metrics['RMSE'])\n",
    "    print(\"Mean Absolute Percentage Error on test set is\", metrics['MAPE'])\n",
    "    print(\"Adjusted R2 score on test set is\", adj_R2)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52181b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on test set is 0.22736186203334408\n",
      "Mean Absolute Error on test set is 20.519154600854115\n",
      "Root Mean Square error on test set is 27.734693681675626\n",
      "Mean Absolute Percentage Error on test set is 0.41503353019106237\n",
      "Adjusted R2 score on test set is 0.22733645925719492\n"
     ]
    }
   ],
   "source": [
    "predictions = TCNmodel(torch.from_numpy(X_test[:,1:]), torch.from_numpy(X_test[:,0]))\n",
    "test_results = show_metrics(y_test, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb10a8c",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9742295",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(TCNmodel.state_dict(), 'TCN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
