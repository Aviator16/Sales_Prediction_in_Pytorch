{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8b9c31",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec64270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f9b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            store  item  sales\n",
       "date                          \n",
       "2013-01-01      1     1     13\n",
       "2013-01-02      1     1     11\n",
       "2013-01-03      1     1     14\n",
       "2013-01-04      1     1     13\n",
       "2013-01-05      1     1     10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', parse_dates=['date'], index_col = ['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd39674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store  item  sales\n",
       "0 2013-01-01      1     1     13\n",
       "1 2013-01-01      7    12     26\n",
       "2 2013-01-01      7    46     27\n",
       "3 2013-01-01      8    12     54\n",
       "4 2013-01-01      9    12     35"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values('date',ascending=True)\n",
    "test = df[df.index.year == 2017]\n",
    "test.reset_index(level=0, inplace= True)\n",
    "train = df[df.index.year != 2017]\n",
    "train.reset_index(level = 0, inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459ff05",
   "metadata": {},
   "source": [
    "# Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b45341",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'year': train['date'].dt.year-2013, 'month': train['date'].dt.month,\n",
    "                           'day': train['date'].dt.day, 'weekday': train['date'].dt.weekday,\n",
    "                           'store': train['store'], 'item': train['item'], 'sales': train['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n",
    "\n",
    "test_data = pd.DataFrame({'year': test['date'].dt.year-2013, 'month': test['date'].dt.month,\n",
    "                           'day': test['date'].dt.day, 'weekday': test['date'].dt.weekday,\n",
    "                           'store': test['store'], 'item': test['item'], 'sales': test['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc87eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month  day  weekday  store  item  sales\n",
      "0     0      1    1        1      1     1     13\n",
      "1     0      1    1        1      7    12     26\n",
      "2     0      1    1        1      7    46     27\n",
      "3     0      1    1        1      8    12     54\n",
      "4     0      1    1        1      9    12     35\n",
      "(730500, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73fb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_data.drop('sales', axis = 1))\n",
    "y = np.array(train_data['sales'])\n",
    "X_test = np.array(test_data.drop('sales', axis = 1))\n",
    "y_test = np.array(test_data['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d91b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X_train, y_train, val_ratio = 0.2, val_year = 3, half_yearly = 1, randomly = True):\n",
    "    \n",
    "    # Splitting randomly\n",
    "    if randomly:\n",
    "        X_tr, y_tr, X_val, y_val = train_test_split(X_train, y_train, test_size = (val_ratio),\n",
    "                                                          random_state = 6, shuffle = True)\n",
    "    else:\n",
    "        if half_yearly == 1:        #if validation data is first 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]   #if not val_year or in last 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)] #if val_year and first 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)]\n",
    "            \n",
    "        else:                       #if validation data is last 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]  #if not val_year or in first 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]  #if val_year and last 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]\n",
    "            \n",
    "        return X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc559b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (730500, 6) (730500,)\n",
      "Validation: (0, 6) (0,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(X, y, False, 0.3, 3, 0)\n",
    "print(\"Training:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe1d3b",
   "metadata": {},
   "source": [
    "# Creating FeedForward Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899a8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 13, 32, 8, 11, 51]\n",
      "[(5, 3), (13, 7), (32, 16), (8, 4), (11, 6), (51, 26)]\n"
     ]
    }
   ],
   "source": [
    "# For embeddings, the thumb rule is, num_embeddings = no. of unique valus in category + 1 \n",
    "# & embedding_dim = min(50,feat_dim(num_embeddings)/2)\n",
    "dims = [np.unique(X_train[:,i]).size+1 for i in range(X_train.shape[1])]\n",
    "print(dims)\n",
    "embedding_dim = [(x, min(50, (x+1)//2)) for x in dims]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896e1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class for Feed Forward Neural Network\n",
    "class NNwithEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_cont, out_size, layers, dp = 0.3):    #n_cont=no. of cont. feat. in dataframe\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        #print(self.embeds)\n",
    "        self.emb_drop = nn.Dropout(dp)\n",
    "        \n",
    "        layer_list = []\n",
    "        n_emb = sum((out for inp,out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "        #print(n_in)\n",
    "        \n",
    "        for i in layers:\n",
    "            layer_list.append(nn.Linear(n_in, i))\n",
    "            layer_list.append(nn.ReLU(inplace = True))\n",
    "            #layer_list.append(nn.BatchNorm1d(i))\n",
    "            layer_list.append(nn.Dropout(dp))\n",
    "            n_in = i\n",
    "        layer_list.append(nn.Linear(layers[-1], out_size))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "        \n",
    "    def forward(self, X_cat, X_cont):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            #print(i,e,X_cat[:5,i])\n",
    "            embeddings.append(e(X_cat[:,i]))\n",
    "        X = torch.cat(embeddings, axis =1)\n",
    "        X = self.emb_drop(X)\n",
    "\n",
    "        X = torch.cat([X, torch.unsqueeze(X_cont,1)], 1)\n",
    "        out = self.layers(X)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        return split_features(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8649dcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNwithEmbeddings(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(13, 7)\n",
       "    (1): Embedding(32, 16)\n",
       "    (2): Embedding(8, 4)\n",
       "    (3): Embedding(11, 6)\n",
       "    (4): Embedding(51, 26)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0, inplace=False)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=60, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0, inplace=False)\n",
       "    (9): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiating neural network model with 3 hidden layers having 512,128 & 32 nodes each\n",
    "NNmodel = NNwithEmbeddings(embedding_dim[1:], 1, 1,[512,128,32], 0)\n",
    "NNmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0165a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function (for optional use)\n",
    "def smape(x,y):\n",
    "    return 100*torch.mean(2*torch.abs(x-y)/(torch.abs(x)+torch.abs(y)))\n",
    "\n",
    "optim = torch.optim.Adam(NNmodel.parameters(), lr = 0.01)       # We use Adam optimizer and Mean squared Error\n",
    "lossfn = F.mse_loss                                             # loss for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450725e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f50a4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, sets, model, X_train, y_train, lossfn, optimizer):\n",
    "    for i in range(sets//2):\n",
    "        for j in range(2):\n",
    "            X_tr, y_tr, X_val, y_val = split_data(X_train, y_train, val_year=i, half_yearly=j, randomly=False)\n",
    "            losses = []\n",
    "            for k in range(epochs):\n",
    "                k+=1\n",
    "                y_pred = NNmodel(torch.from_numpy(X_tr[:,1:]), torch.from_numpy(X_tr[:,0]))\n",
    "                y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n",
    "                loss = lossfn(y_pred,torch.tensor(y_tr))\n",
    "                losses.append(loss)\n",
    "                #if k%2 == 1:\n",
    "                print(\"Epoch number {} of validation year 20{} and half {} has MSE loss {}\".format(k,13+i,j,loss.item()))\n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if k%2 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        yhat_val = NNmodel(torch.from_numpy(X_val[:,1:]), torch.from_numpy(X_val[:,0]))\n",
    "                        y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n",
    "                        val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n",
    "                    print(\"Validation loss at epoch {} of year 20{} half {} is {:.4f}\".format(k,13+i,j,val_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833d5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/3281674988.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = lossfn(y_pred,torch.tensor(y_tr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1 of validation year 2013 and half 0 has MSE loss 3435.637451171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/3281674988.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 2 of validation year 2013 and half 0 has MSE loss 3194.018310546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/3281674988.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 2 of year 2013 half 0 is 44.5179\n",
      "Epoch number 3 of validation year 2013 and half 0 has MSE loss 2648.561767578125\n",
      "Epoch number 4 of validation year 2013 and half 0 has MSE loss 1691.4736328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5301/3281674988.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 4 of year 2013 half 0 is 23.9613\n",
      "Epoch number 5 of validation year 2013 and half 0 has MSE loss 778.6845703125\n",
      "Epoch number 6 of validation year 2013 and half 0 has MSE loss 1949.109619140625\n",
      "Validation loss at epoch 6 of year 2013 half 0 is 31.4415\n",
      "Epoch number 7 of validation year 2013 and half 0 has MSE loss 1119.71826171875\n",
      "Epoch number 8 of validation year 2013 and half 0 has MSE loss 612.288330078125\n",
      "Validation loss at epoch 8 of year 2013 half 0 is 21.7779\n",
      "Epoch number 9 of validation year 2013 and half 0 has MSE loss 695.399658203125\n",
      "Epoch number 10 of validation year 2013 and half 0 has MSE loss 886.1011962890625\n",
      "Validation loss at epoch 10 of year 2013 half 0 is 25.5493\n",
      "Epoch number 11 of validation year 2013 and half 0 has MSE loss 969.1436157226562\n",
      "Epoch number 12 of validation year 2013 and half 0 has MSE loss 913.7047119140625\n",
      "Validation loss at epoch 12 of year 2013 half 0 is 22.1434\n",
      "Epoch number 13 of validation year 2013 and half 0 has MSE loss 750.3989868164062\n",
      "Epoch number 14 of validation year 2013 and half 0 has MSE loss 549.4110107421875\n",
      "Validation loss at epoch 14 of year 2013 half 0 is 17.7857\n",
      "Epoch number 15 of validation year 2013 and half 0 has MSE loss 424.6375427246094\n",
      "Epoch number 16 of validation year 2013 and half 0 has MSE loss 470.2682800292969\n",
      "Validation loss at epoch 16 of year 2013 half 0 is 24.2120\n",
      "Epoch number 17 of validation year 2013 and half 0 has MSE loss 587.431396484375\n",
      "Epoch number 18 of validation year 2013 and half 0 has MSE loss 552.6255493164062\n",
      "Validation loss at epoch 18 of year 2013 half 0 is 19.6642\n",
      "Epoch number 19 of validation year 2013 and half 0 has MSE loss 394.68084716796875\n",
      "Epoch number 20 of validation year 2013 and half 0 has MSE loss 291.73248291015625\n",
      "Validation loss at epoch 20 of year 2013 half 0 is 14.1994\n",
      "Epoch number 21 of validation year 2013 and half 0 has MSE loss 293.6403503417969\n",
      "Epoch number 22 of validation year 2013 and half 0 has MSE loss 337.64642333984375\n",
      "Validation loss at epoch 22 of year 2013 half 0 is 14.8341\n",
      "Epoch number 23 of validation year 2013 and half 0 has MSE loss 358.911376953125\n",
      "Epoch number 24 of validation year 2013 and half 0 has MSE loss 333.2080078125\n",
      "Validation loss at epoch 24 of year 2013 half 0 is 13.3261\n",
      "Epoch number 25 of validation year 2013 and half 0 has MSE loss 274.0907287597656\n",
      "Epoch number 26 of validation year 2013 and half 0 has MSE loss 219.6555938720703\n",
      "Validation loss at epoch 26 of year 2013 half 0 is 14.1887\n",
      "Epoch number 27 of validation year 2013 and half 0 has MSE loss 207.69200134277344\n",
      "Epoch number 28 of validation year 2013 and half 0 has MSE loss 235.4602813720703\n",
      "Validation loss at epoch 28 of year 2013 half 0 is 17.2008\n",
      "Epoch number 29 of validation year 2013 and half 0 has MSE loss 250.5145721435547\n",
      "Epoch number 30 of validation year 2013 and half 0 has MSE loss 220.44847106933594\n",
      "Validation loss at epoch 30 of year 2013 half 0 is 13.9532\n",
      "Epoch number 31 of validation year 2013 and half 0 has MSE loss 178.0386962890625\n",
      "Epoch number 32 of validation year 2013 and half 0 has MSE loss 163.56651306152344\n",
      "Validation loss at epoch 32 of year 2013 half 0 is 11.4143\n",
      "Epoch number 1 of validation year 2013 and half 1 has MSE loss 179.03704833984375\n",
      "Epoch number 2 of validation year 2013 and half 1 has MSE loss 192.7178192138672\n",
      "Validation loss at epoch 2 of year 2013 half 1 is 10.3555\n",
      "Epoch number 3 of validation year 2013 and half 1 has MSE loss 189.59585571289062\n",
      "Epoch number 4 of validation year 2013 and half 1 has MSE loss 170.90444946289062\n",
      "Validation loss at epoch 4 of year 2013 half 1 is 10.5542\n",
      "Epoch number 5 of validation year 2013 and half 1 has MSE loss 152.8395233154297\n",
      "Epoch number 6 of validation year 2013 and half 1 has MSE loss 150.39724731445312\n",
      "Validation loss at epoch 6 of year 2013 half 1 is 12.3269\n",
      "Epoch number 7 of validation year 2013 and half 1 has MSE loss 160.03614807128906\n",
      "Epoch number 8 of validation year 2013 and half 1 has MSE loss 162.78660583496094\n",
      "Validation loss at epoch 8 of year 2013 half 1 is 11.9837\n",
      "Epoch number 9 of validation year 2013 and half 1 has MSE loss 150.83375549316406\n",
      "Epoch number 10 of validation year 2013 and half 1 has MSE loss 137.19874572753906\n",
      "Validation loss at epoch 10 of year 2013 half 1 is 10.1047\n",
      "Epoch number 11 of validation year 2013 and half 1 has MSE loss 134.11431884765625\n",
      "Epoch number 12 of validation year 2013 and half 1 has MSE loss 138.27029418945312\n",
      "Validation loss at epoch 12 of year 2013 half 1 is 9.5183\n",
      "Epoch number 13 of validation year 2013 and half 1 has MSE loss 139.19261169433594\n",
      "Epoch number 14 of validation year 2013 and half 1 has MSE loss 132.39962768554688\n",
      "Validation loss at epoch 14 of year 2013 half 1 is 9.6823\n",
      "Epoch number 15 of validation year 2013 and half 1 has MSE loss 122.92790985107422\n",
      "Epoch number 16 of validation year 2013 and half 1 has MSE loss 118.65515899658203\n",
      "Validation loss at epoch 16 of year 2013 half 1 is 10.8388\n",
      "Epoch number 17 of validation year 2013 and half 1 has MSE loss 120.58497619628906\n",
      "Epoch number 18 of validation year 2013 and half 1 has MSE loss 121.59705352783203\n",
      "Validation loss at epoch 18 of year 2013 half 1 is 10.7855\n",
      "Epoch number 19 of validation year 2013 and half 1 has MSE loss 116.8840103149414\n",
      "Epoch number 20 of validation year 2013 and half 1 has MSE loss 110.52960205078125\n",
      "Validation loss at epoch 20 of year 2013 half 1 is 9.4627\n",
      "Epoch number 21 of validation year 2013 and half 1 has MSE loss 108.12987518310547\n",
      "Epoch number 22 of validation year 2013 and half 1 has MSE loss 108.9371109008789\n",
      "Validation loss at epoch 22 of year 2013 half 1 is 8.9095\n",
      "Epoch number 23 of validation year 2013 and half 1 has MSE loss 108.3824234008789\n",
      "Epoch number 24 of validation year 2013 and half 1 has MSE loss 104.61827850341797\n",
      "Validation loss at epoch 24 of year 2013 half 1 is 9.1634\n",
      "Epoch number 25 of validation year 2013 and half 1 has MSE loss 100.32107543945312\n",
      "Epoch number 26 of validation year 2013 and half 1 has MSE loss 98.6863784790039\n",
      "Validation loss at epoch 26 of year 2013 half 1 is 9.8067\n",
      "Epoch number 27 of validation year 2013 and half 1 has MSE loss 99.03508758544922\n",
      "Epoch number 28 of validation year 2013 and half 1 has MSE loss 98.08979797363281\n",
      "Validation loss at epoch 28 of year 2013 half 1 is 9.4762\n",
      "Epoch number 29 of validation year 2013 and half 1 has MSE loss 95.0967025756836\n",
      "Epoch number 30 of validation year 2013 and half 1 has MSE loss 92.55410766601562\n",
      "Validation loss at epoch 30 of year 2013 half 1 is 8.6845\n",
      "Epoch number 31 of validation year 2013 and half 1 has MSE loss 91.93486785888672\n",
      "Epoch number 32 of validation year 2013 and half 1 has MSE loss 91.80886840820312\n",
      "Validation loss at epoch 32 of year 2013 half 1 is 8.4650\n",
      "Epoch number 1 of validation year 2014 and half 0 has MSE loss 87.960693359375\n",
      "Epoch number 2 of validation year 2014 and half 0 has MSE loss 86.53665924072266\n",
      "Validation loss at epoch 2 of year 2014 half 0 is 9.1620\n",
      "Epoch number 3 of validation year 2014 and half 0 has MSE loss 85.3184814453125\n",
      "Epoch number 4 of validation year 2014 and half 0 has MSE loss 84.3525619506836\n",
      "Validation loss at epoch 4 of year 2014 half 0 is 8.9797\n",
      "Epoch number 5 of validation year 2014 and half 0 has MSE loss 83.32988739013672\n",
      "Epoch number 6 of validation year 2014 and half 0 has MSE loss 82.09412384033203\n",
      "Validation loss at epoch 6 of year 2014 half 0 is 8.9500\n",
      "Epoch number 7 of validation year 2014 and half 0 has MSE loss 80.861572265625\n",
      "Epoch number 8 of validation year 2014 and half 0 has MSE loss 79.85838317871094\n",
      "Validation loss at epoch 8 of year 2014 half 0 is 8.9743\n",
      "Epoch number 9 of validation year 2014 and half 0 has MSE loss 79.0037612915039\n",
      "Epoch number 10 of validation year 2014 and half 0 has MSE loss 78.08946228027344\n",
      "Validation loss at epoch 10 of year 2014 half 0 is 8.8603\n",
      "Epoch number 11 of validation year 2014 and half 0 has MSE loss 77.10151672363281\n",
      "Epoch number 12 of validation year 2014 and half 0 has MSE loss 76.19940185546875\n",
      "Validation loss at epoch 12 of year 2014 half 0 is 8.6879\n",
      "Epoch number 13 of validation year 2014 and half 0 has MSE loss 75.4419937133789\n",
      "Epoch number 14 of validation year 2014 and half 0 has MSE loss 74.70545196533203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 14 of year 2014 half 0 is 8.6136\n",
      "Epoch number 15 of validation year 2014 and half 0 has MSE loss 73.89508819580078\n",
      "Epoch number 16 of validation year 2014 and half 0 has MSE loss 73.08456420898438\n",
      "Validation loss at epoch 16 of year 2014 half 0 is 8.6316\n",
      "Epoch number 17 of validation year 2014 and half 0 has MSE loss 72.37355041503906\n",
      "Epoch number 18 of validation year 2014 and half 0 has MSE loss 71.73001861572266\n",
      "Validation loss at epoch 18 of year 2014 half 0 is 8.6048\n",
      "Epoch number 19 of validation year 2014 and half 0 has MSE loss 71.06254577636719\n",
      "Epoch number 20 of validation year 2014 and half 0 has MSE loss 70.37023162841797\n",
      "Validation loss at epoch 20 of year 2014 half 0 is 8.4783\n",
      "Epoch number 21 of validation year 2014 and half 0 has MSE loss 69.72797393798828\n",
      "Epoch number 22 of validation year 2014 and half 0 has MSE loss 69.15701293945312\n",
      "Validation loss at epoch 22 of year 2014 half 0 is 8.3834\n",
      "Epoch number 23 of validation year 2014 and half 0 has MSE loss 68.60066986083984\n",
      "Epoch number 24 of validation year 2014 and half 0 has MSE loss 68.02849578857422\n",
      "Validation loss at epoch 24 of year 2014 half 0 is 8.3802\n",
      "Epoch number 25 of validation year 2014 and half 0 has MSE loss 67.48110961914062\n",
      "Epoch number 26 of validation year 2014 and half 0 has MSE loss 66.9917221069336\n",
      "Validation loss at epoch 26 of year 2014 half 0 is 8.3892\n",
      "Epoch number 27 of validation year 2014 and half 0 has MSE loss 66.53258514404297\n",
      "Epoch number 28 of validation year 2014 and half 0 has MSE loss 66.06928253173828\n",
      "Validation loss at epoch 28 of year 2014 half 0 is 8.3224\n",
      "Epoch number 29 of validation year 2014 and half 0 has MSE loss 65.61675262451172\n",
      "Epoch number 30 of validation year 2014 and half 0 has MSE loss 65.2061767578125\n",
      "Validation loss at epoch 30 of year 2014 half 0 is 8.2446\n",
      "Epoch number 31 of validation year 2014 and half 0 has MSE loss 64.83070373535156\n",
      "Epoch number 32 of validation year 2014 and half 0 has MSE loss 64.46259307861328\n",
      "Validation loss at epoch 32 of year 2014 half 0 is 8.2339\n",
      "Epoch number 1 of validation year 2014 and half 1 has MSE loss 65.06417846679688\n",
      "Epoch number 2 of validation year 2014 and half 1 has MSE loss 64.72937774658203\n",
      "Validation loss at epoch 2 of year 2014 half 1 is 7.8021\n",
      "Epoch number 3 of validation year 2014 and half 1 has MSE loss 64.39271545410156\n",
      "Epoch number 4 of validation year 2014 and half 1 has MSE loss 64.0615234375\n",
      "Validation loss at epoch 4 of year 2014 half 1 is 7.7802\n",
      "Epoch number 5 of validation year 2014 and half 1 has MSE loss 63.75144958496094\n",
      "Epoch number 6 of validation year 2014 and half 1 has MSE loss 63.46749496459961\n",
      "Validation loss at epoch 6 of year 2014 half 1 is 7.7802\n",
      "Epoch number 7 of validation year 2014 and half 1 has MSE loss 63.203575134277344\n",
      "Epoch number 8 of validation year 2014 and half 1 has MSE loss 62.95698165893555\n",
      "Validation loss at epoch 8 of year 2014 half 1 is 7.8103\n",
      "Epoch number 9 of validation year 2014 and half 1 has MSE loss 62.730018615722656\n",
      "Epoch number 10 of validation year 2014 and half 1 has MSE loss 62.51945114135742\n",
      "Validation loss at epoch 10 of year 2014 half 1 is 7.8269\n",
      "Epoch number 11 of validation year 2014 and half 1 has MSE loss 62.316593170166016\n",
      "Epoch number 12 of validation year 2014 and half 1 has MSE loss 62.117698669433594\n",
      "Validation loss at epoch 12 of year 2014 half 1 is 7.8060\n",
      "Epoch number 13 of validation year 2014 and half 1 has MSE loss 61.92551803588867\n",
      "Epoch number 14 of validation year 2014 and half 1 has MSE loss 61.74100112915039\n",
      "Validation loss at epoch 14 of year 2014 half 1 is 7.7810\n",
      "Epoch number 15 of validation year 2014 and half 1 has MSE loss 61.56148147583008\n",
      "Epoch number 16 of validation year 2014 and half 1 has MSE loss 61.38695526123047\n",
      "Validation loss at epoch 16 of year 2014 half 1 is 7.7718\n",
      "Epoch number 17 of validation year 2014 and half 1 has MSE loss 61.220943450927734\n",
      "Epoch number 18 of validation year 2014 and half 1 has MSE loss 61.064361572265625\n",
      "Validation loss at epoch 18 of year 2014 half 1 is 7.7573\n",
      "Epoch number 19 of validation year 2014 and half 1 has MSE loss 60.914466857910156\n",
      "Epoch number 20 of validation year 2014 and half 1 has MSE loss 60.76940155029297\n",
      "Validation loss at epoch 20 of year 2014 half 1 is 7.7296\n",
      "Epoch number 21 of validation year 2014 and half 1 has MSE loss 60.629180908203125\n",
      "Epoch number 22 of validation year 2014 and half 1 has MSE loss 60.49238586425781\n",
      "Validation loss at epoch 22 of year 2014 half 1 is 7.7132\n",
      "Epoch number 23 of validation year 2014 and half 1 has MSE loss 60.35613250732422\n",
      "Epoch number 24 of validation year 2014 and half 1 has MSE loss 60.219573974609375\n",
      "Validation loss at epoch 24 of year 2014 half 1 is 7.7169\n",
      "Epoch number 25 of validation year 2014 and half 1 has MSE loss 60.08416748046875\n",
      "Epoch number 26 of validation year 2014 and half 1 has MSE loss 59.95067596435547\n",
      "Validation loss at epoch 26 of year 2014 half 1 is 7.7195\n",
      "Epoch number 27 of validation year 2014 and half 1 has MSE loss 59.818824768066406\n",
      "Epoch number 28 of validation year 2014 and half 1 has MSE loss 59.68911361694336\n",
      "Validation loss at epoch 28 of year 2014 half 1 is 7.7113\n",
      "Epoch number 29 of validation year 2014 and half 1 has MSE loss 59.56266784667969\n",
      "Epoch number 30 of validation year 2014 and half 1 has MSE loss 59.439273834228516\n",
      "Validation loss at epoch 30 of year 2014 half 1 is 7.7065\n",
      "Epoch number 31 of validation year 2014 and half 1 has MSE loss 59.317867279052734\n",
      "Epoch number 32 of validation year 2014 and half 1 has MSE loss 59.19816970825195\n",
      "Validation loss at epoch 32 of year 2014 half 1 is 7.7074\n",
      "Epoch number 1 of validation year 2015 and half 0 has MSE loss 58.815311431884766\n",
      "Epoch number 2 of validation year 2015 and half 0 has MSE loss 58.60274887084961\n",
      "Validation loss at epoch 2 of year 2015 half 0 is 7.8354\n",
      "Epoch number 3 of validation year 2015 and half 0 has MSE loss 58.461055755615234\n",
      "Epoch number 4 of validation year 2015 and half 0 has MSE loss 58.400123596191406\n",
      "Validation loss at epoch 4 of year 2015 half 0 is 7.8329\n",
      "Epoch number 5 of validation year 2015 and half 0 has MSE loss 58.273616790771484\n",
      "Epoch number 6 of validation year 2015 and half 0 has MSE loss 58.09346389770508\n",
      "Validation loss at epoch 6 of year 2015 half 0 is 7.7904\n",
      "Epoch number 7 of validation year 2015 and half 0 has MSE loss 57.983619689941406\n",
      "Epoch number 8 of validation year 2015 and half 0 has MSE loss 57.92363357543945\n",
      "Validation loss at epoch 8 of year 2015 half 0 is 7.7763\n",
      "Epoch number 9 of validation year 2015 and half 0 has MSE loss 57.813751220703125\n",
      "Epoch number 10 of validation year 2015 and half 0 has MSE loss 57.676979064941406\n",
      "Validation loss at epoch 10 of year 2015 half 0 is 7.7738\n",
      "Epoch number 11 of validation year 2015 and half 0 has MSE loss 57.58964920043945\n",
      "Epoch number 12 of validation year 2015 and half 0 has MSE loss 57.521846771240234\n",
      "Validation loss at epoch 12 of year 2015 half 0 is 7.7646\n",
      "Epoch number 13 of validation year 2015 and half 0 has MSE loss 57.41267395019531\n",
      "Epoch number 14 of validation year 2015 and half 0 has MSE loss 57.2951545715332\n",
      "Validation loss at epoch 14 of year 2015 half 0 is 7.7438\n",
      "Epoch number 15 of validation year 2015 and half 0 has MSE loss 57.21592330932617\n",
      "Epoch number 16 of validation year 2015 and half 0 has MSE loss 57.14312744140625\n",
      "Validation loss at epoch 16 of year 2015 half 0 is 7.7329\n",
      "Epoch number 17 of validation year 2015 and half 0 has MSE loss 57.04463577270508\n",
      "Epoch number 18 of validation year 2015 and half 0 has MSE loss 56.951473236083984\n",
      "Validation loss at epoch 18 of year 2015 half 0 is 7.7313\n",
      "Epoch number 19 of validation year 2015 and half 0 has MSE loss 56.88486099243164\n",
      "Epoch number 20 of validation year 2015 and half 0 has MSE loss 56.814483642578125\n",
      "Validation loss at epoch 20 of year 2015 half 0 is 7.7193\n",
      "Epoch number 21 of validation year 2015 and half 0 has MSE loss 56.72650146484375\n",
      "Epoch number 22 of validation year 2015 and half 0 has MSE loss 56.64716720581055\n",
      "Validation loss at epoch 22 of year 2015 half 0 is 7.7028\n",
      "Epoch number 23 of validation year 2015 and half 0 has MSE loss 56.58243942260742\n",
      "Epoch number 24 of validation year 2015 and half 0 has MSE loss 56.50981903076172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 24 of year 2015 half 0 is 7.6949\n",
      "Epoch number 25 of validation year 2015 and half 0 has MSE loss 56.42906951904297\n",
      "Epoch number 26 of validation year 2015 and half 0 has MSE loss 56.35916519165039\n",
      "Validation loss at epoch 26 of year 2015 half 0 is 7.6907\n",
      "Epoch number 27 of validation year 2015 and half 0 has MSE loss 56.29690933227539\n",
      "Epoch number 28 of validation year 2015 and half 0 has MSE loss 56.22747802734375\n",
      "Validation loss at epoch 28 of year 2015 half 0 is 7.6796\n",
      "Epoch number 29 of validation year 2015 and half 0 has MSE loss 56.1566162109375\n",
      "Epoch number 30 of validation year 2015 and half 0 has MSE loss 56.09492874145508\n",
      "Validation loss at epoch 30 of year 2015 half 0 is 7.6702\n",
      "Epoch number 31 of validation year 2015 and half 0 has MSE loss 56.03496170043945\n",
      "Epoch number 32 of validation year 2015 and half 0 has MSE loss 55.96944808959961\n",
      "Validation loss at epoch 32 of year 2015 half 0 is 7.6657\n",
      "Epoch number 1 of validation year 2015 and half 1 has MSE loss 56.66518020629883\n",
      "Epoch number 2 of validation year 2015 and half 1 has MSE loss 56.60842514038086\n",
      "Validation loss at epoch 2 of year 2015 half 1 is 7.3053\n",
      "Epoch number 3 of validation year 2015 and half 1 has MSE loss 56.54973602294922\n",
      "Epoch number 4 of validation year 2015 and half 1 has MSE loss 56.48737335205078\n",
      "Validation loss at epoch 4 of year 2015 half 1 is 7.2959\n",
      "Epoch number 5 of validation year 2015 and half 1 has MSE loss 56.42882537841797\n",
      "Epoch number 6 of validation year 2015 and half 1 has MSE loss 56.373756408691406\n",
      "Validation loss at epoch 6 of year 2015 half 1 is 7.2898\n",
      "Epoch number 7 of validation year 2015 and half 1 has MSE loss 56.316368103027344\n",
      "Epoch number 8 of validation year 2015 and half 1 has MSE loss 56.25880813598633\n",
      "Validation loss at epoch 8 of year 2015 half 1 is 7.2878\n",
      "Epoch number 9 of validation year 2015 and half 1 has MSE loss 56.20512008666992\n",
      "Epoch number 10 of validation year 2015 and half 1 has MSE loss 56.15229415893555\n",
      "Validation loss at epoch 10 of year 2015 half 1 is 7.2827\n",
      "Epoch number 11 of validation year 2015 and half 1 has MSE loss 56.098114013671875\n",
      "Epoch number 12 of validation year 2015 and half 1 has MSE loss 56.04584884643555\n",
      "Validation loss at epoch 12 of year 2015 half 1 is 7.2754\n",
      "Epoch number 13 of validation year 2015 and half 1 has MSE loss 55.99605178833008\n",
      "Epoch number 14 of validation year 2015 and half 1 has MSE loss 55.94568634033203\n",
      "Validation loss at epoch 14 of year 2015 half 1 is 7.2724\n",
      "Epoch number 15 of validation year 2015 and half 1 has MSE loss 55.89535140991211\n",
      "Epoch number 16 of validation year 2015 and half 1 has MSE loss 55.847312927246094\n",
      "Validation loss at epoch 16 of year 2015 half 1 is 7.2703\n",
      "Epoch number 17 of validation year 2015 and half 1 has MSE loss 55.800331115722656\n",
      "Epoch number 18 of validation year 2015 and half 1 has MSE loss 55.75304412841797\n",
      "Validation loss at epoch 18 of year 2015 half 1 is 7.2646\n",
      "Epoch number 19 of validation year 2015 and half 1 has MSE loss 55.70698928833008\n",
      "Epoch number 20 of validation year 2015 and half 1 has MSE loss 55.66261291503906\n",
      "Validation loss at epoch 20 of year 2015 half 1 is 7.2601\n",
      "Epoch number 21 of validation year 2015 and half 1 has MSE loss 55.61844253540039\n",
      "Epoch number 22 of validation year 2015 and half 1 has MSE loss 55.574642181396484\n",
      "Validation loss at epoch 22 of year 2015 half 1 is 7.2583\n",
      "Epoch number 23 of validation year 2015 and half 1 has MSE loss 55.53226852416992\n",
      "Epoch number 24 of validation year 2015 and half 1 has MSE loss 55.49066162109375\n",
      "Validation loss at epoch 24 of year 2015 half 1 is 7.2544\n",
      "Epoch number 25 of validation year 2015 and half 1 has MSE loss 55.449092864990234\n",
      "Epoch number 26 of validation year 2015 and half 1 has MSE loss 55.40834426879883\n",
      "Validation loss at epoch 26 of year 2015 half 1 is 7.2491\n",
      "Epoch number 27 of validation year 2015 and half 1 has MSE loss 55.36861801147461\n",
      "Epoch number 28 of validation year 2015 and half 1 has MSE loss 55.329219818115234\n",
      "Validation loss at epoch 28 of year 2015 half 1 is 7.2462\n",
      "Epoch number 29 of validation year 2015 and half 1 has MSE loss 55.29029846191406\n",
      "Epoch number 30 of validation year 2015 and half 1 has MSE loss 55.252349853515625\n",
      "Validation loss at epoch 30 of year 2015 half 1 is 7.2434\n",
      "Epoch number 31 of validation year 2015 and half 1 has MSE loss 55.214969635009766\n",
      "Epoch number 32 of validation year 2015 and half 1 has MSE loss 55.17789840698242\n",
      "Validation loss at epoch 32 of year 2015 half 1 is 7.2385\n",
      "Epoch number 1 of validation year 2016 and half 0 has MSE loss 53.4008903503418\n",
      "Epoch number 2 of validation year 2016 and half 0 has MSE loss 53.361045837402344\n",
      "Validation loss at epoch 2 of year 2016 half 0 is 8.0333\n",
      "Epoch number 3 of validation year 2016 and half 0 has MSE loss 53.321510314941406\n",
      "Epoch number 4 of validation year 2016 and half 0 has MSE loss 53.28079605102539\n",
      "Validation loss at epoch 4 of year 2016 half 0 is 8.0417\n",
      "Epoch number 5 of validation year 2016 and half 0 has MSE loss 53.23918533325195\n",
      "Epoch number 6 of validation year 2016 and half 0 has MSE loss 53.20240020751953\n",
      "Validation loss at epoch 6 of year 2016 half 0 is 8.0539\n",
      "Epoch number 7 of validation year 2016 and half 0 has MSE loss 53.17024230957031\n",
      "Epoch number 8 of validation year 2016 and half 0 has MSE loss 53.13836669921875\n",
      "Validation loss at epoch 8 of year 2016 half 0 is 8.0628\n",
      "Epoch number 9 of validation year 2016 and half 0 has MSE loss 53.10758972167969\n",
      "Epoch number 10 of validation year 2016 and half 0 has MSE loss 53.07944869995117\n",
      "Validation loss at epoch 10 of year 2016 half 0 is 8.0667\n",
      "Epoch number 11 of validation year 2016 and half 0 has MSE loss 53.051063537597656\n",
      "Epoch number 12 of validation year 2016 and half 0 has MSE loss 53.021305084228516\n",
      "Validation loss at epoch 12 of year 2016 half 0 is 8.0675\n",
      "Epoch number 13 of validation year 2016 and half 0 has MSE loss 52.99262619018555\n",
      "Epoch number 14 of validation year 2016 and half 0 has MSE loss 52.965145111083984\n",
      "Validation loss at epoch 14 of year 2016 half 0 is 8.0641\n",
      "Epoch number 15 of validation year 2016 and half 0 has MSE loss 52.93742370605469\n",
      "Epoch number 16 of validation year 2016 and half 0 has MSE loss 52.910667419433594\n",
      "Validation loss at epoch 16 of year 2016 half 0 is 8.0573\n",
      "Epoch number 17 of validation year 2016 and half 0 has MSE loss 52.8858757019043\n",
      "Epoch number 18 of validation year 2016 and half 0 has MSE loss 52.86164474487305\n",
      "Validation loss at epoch 18 of year 2016 half 0 is 8.0528\n",
      "Epoch number 19 of validation year 2016 and half 0 has MSE loss 52.837501525878906\n",
      "Epoch number 20 of validation year 2016 and half 0 has MSE loss 52.814388275146484\n",
      "Validation loss at epoch 20 of year 2016 half 0 is 8.0504\n",
      "Epoch number 21 of validation year 2016 and half 0 has MSE loss 52.79176712036133\n",
      "Epoch number 22 of validation year 2016 and half 0 has MSE loss 52.76870346069336\n",
      "Validation loss at epoch 22 of year 2016 half 0 is 8.0470\n",
      "Epoch number 23 of validation year 2016 and half 0 has MSE loss 52.7458610534668\n",
      "Epoch number 24 of validation year 2016 and half 0 has MSE loss 52.7236328125\n",
      "Validation loss at epoch 24 of year 2016 half 0 is 8.0453\n",
      "Epoch number 25 of validation year 2016 and half 0 has MSE loss 52.70146179199219\n",
      "Epoch number 26 of validation year 2016 and half 0 has MSE loss 52.679588317871094\n",
      "Validation loss at epoch 26 of year 2016 half 0 is 8.0457\n",
      "Epoch number 27 of validation year 2016 and half 0 has MSE loss 52.658573150634766\n",
      "Epoch number 28 of validation year 2016 and half 0 has MSE loss 52.63806915283203\n",
      "Validation loss at epoch 28 of year 2016 half 0 is 8.0441\n",
      "Epoch number 29 of validation year 2016 and half 0 has MSE loss 52.617759704589844\n",
      "Epoch number 30 of validation year 2016 and half 0 has MSE loss 52.59799575805664\n",
      "Validation loss at epoch 30 of year 2016 half 0 is 8.0407\n",
      "Epoch number 31 of validation year 2016 and half 0 has MSE loss 52.57869338989258\n",
      "Epoch number 32 of validation year 2016 and half 0 has MSE loss 52.559478759765625\n",
      "Validation loss at epoch 32 of year 2016 half 0 is 8.0379\n",
      "Epoch number 1 of validation year 2016 and half 1 has MSE loss 53.60689163208008\n",
      "Epoch number 2 of validation year 2016 and half 1 has MSE loss 53.58001708984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 2 of year 2016 half 1 is 7.5626\n",
      "Epoch number 3 of validation year 2016 and half 1 has MSE loss 53.54899597167969\n",
      "Epoch number 4 of validation year 2016 and half 1 has MSE loss 53.5174674987793\n",
      "Validation loss at epoch 4 of year 2016 half 1 is 7.5600\n",
      "Epoch number 5 of validation year 2016 and half 1 has MSE loss 53.48748779296875\n",
      "Epoch number 6 of validation year 2016 and half 1 has MSE loss 53.45970916748047\n",
      "Validation loss at epoch 6 of year 2016 half 1 is 7.5605\n",
      "Epoch number 7 of validation year 2016 and half 1 has MSE loss 53.433982849121094\n",
      "Epoch number 8 of validation year 2016 and half 1 has MSE loss 53.4094123840332\n",
      "Validation loss at epoch 8 of year 2016 half 1 is 7.5617\n",
      "Epoch number 9 of validation year 2016 and half 1 has MSE loss 53.385162353515625\n",
      "Epoch number 10 of validation year 2016 and half 1 has MSE loss 53.36107635498047\n",
      "Validation loss at epoch 10 of year 2016 half 1 is 7.5633\n",
      "Epoch number 11 of validation year 2016 and half 1 has MSE loss 53.33723068237305\n",
      "Epoch number 12 of validation year 2016 and half 1 has MSE loss 53.313812255859375\n",
      "Validation loss at epoch 12 of year 2016 half 1 is 7.5676\n",
      "Epoch number 13 of validation year 2016 and half 1 has MSE loss 53.291263580322266\n",
      "Epoch number 14 of validation year 2016 and half 1 has MSE loss 53.26990509033203\n",
      "Validation loss at epoch 14 of year 2016 half 1 is 7.5725\n",
      "Epoch number 15 of validation year 2016 and half 1 has MSE loss 53.24965286254883\n",
      "Epoch number 16 of validation year 2016 and half 1 has MSE loss 53.23027420043945\n",
      "Validation loss at epoch 16 of year 2016 half 1 is 7.5760\n",
      "Epoch number 17 of validation year 2016 and half 1 has MSE loss 53.211483001708984\n",
      "Epoch number 18 of validation year 2016 and half 1 has MSE loss 53.1928825378418\n",
      "Validation loss at epoch 18 of year 2016 half 1 is 7.5792\n",
      "Epoch number 19 of validation year 2016 and half 1 has MSE loss 53.1742057800293\n",
      "Epoch number 20 of validation year 2016 and half 1 has MSE loss 53.155513763427734\n",
      "Validation loss at epoch 20 of year 2016 half 1 is 7.5810\n",
      "Epoch number 21 of validation year 2016 and half 1 has MSE loss 53.136863708496094\n",
      "Epoch number 22 of validation year 2016 and half 1 has MSE loss 53.1182975769043\n",
      "Validation loss at epoch 22 of year 2016 half 1 is 7.5799\n",
      "Epoch number 23 of validation year 2016 and half 1 has MSE loss 53.09991455078125\n",
      "Epoch number 24 of validation year 2016 and half 1 has MSE loss 53.08173370361328\n",
      "Validation loss at epoch 24 of year 2016 half 1 is 7.5778\n",
      "Epoch number 25 of validation year 2016 and half 1 has MSE loss 53.06370544433594\n",
      "Epoch number 26 of validation year 2016 and half 1 has MSE loss 53.04591751098633\n",
      "Validation loss at epoch 26 of year 2016 half 1 is 7.5752\n",
      "Epoch number 27 of validation year 2016 and half 1 has MSE loss 53.028446197509766\n",
      "Epoch number 28 of validation year 2016 and half 1 has MSE loss 53.01128387451172\n",
      "Validation loss at epoch 28 of year 2016 half 1 is 7.5714\n",
      "Epoch number 29 of validation year 2016 and half 1 has MSE loss 52.99442672729492\n",
      "Epoch number 30 of validation year 2016 and half 1 has MSE loss 52.977882385253906\n",
      "Validation loss at epoch 30 of year 2016 half 1 is 7.5677\n",
      "Epoch number 31 of validation year 2016 and half 1 has MSE loss 52.961551666259766\n",
      "Epoch number 32 of validation year 2016 and half 1 has MSE loss 52.94541549682617\n",
      "Validation loss at epoch 32 of year 2016 half 1 is 7.5650\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "begin = time.time()\n",
    "fit(32, 8, NNmodel, X, y, lossfn, optim)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b7b2e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training in a Ryzen 5 Hexa core 4600H CPU is 30.64 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken for training in a Ryzen 5 Hexa core 4600H CPU is {:.2f} minutes\".format((end-begin)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6ed21",
   "metadata": {},
   "source": [
    "\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c036cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "#import torch_metrics as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bdd7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(test, y_pred_final):\n",
    "    metrics = {'R2_score': r2_score(test, y_pred_final), 'MAE': mean_absolute_error(test, y_pred_final),\n",
    "               'RMSE': mean_squared_error(test, y_pred_final, squared=False),\n",
    "               'MAPE': mean_absolute_percentage_error(test, y_pred_final)}\n",
    "    adj_R2 = 1-(1-metrics['R2_score'])*(len(test)-1)/(len(test)-6-1)      #num of indep var = 6\n",
    "    metrics['adj_R2'] = adj_R2\n",
    "    print(\"R2 score on test set is\", metrics['R2_score'])\n",
    "    print(\"Mean Absolute Error on test set is\", metrics['MAE'])\n",
    "    print(\"Root Mean Square error on test set is\", metrics['RMSE'])\n",
    "    print(\"Mean Absolute Percentage Error on test set is\", metrics['MAPE'])\n",
    "    print(\"Adjusted R2 score on test set is\", adj_R2)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52181b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on test set is 0.923525640363284\n",
      "Mean Absolute Error on test set is 6.846392510761627\n",
      "Root Mean Square error on test set is 8.725558357325898\n",
      "Mean Absolute Percentage Error on test set is 0.15158092560938577\n",
      "Adjusted R2 score on test set is 0.9235231260413219\n"
     ]
    }
   ],
   "source": [
    "predictions = NNmodel(torch.from_numpy(X_test[:,1:]), torch.from_numpy(X_test[:,0]))\n",
    "test_results = show_metrics(y_test, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb10a8c",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9742295",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(NNmodel.state_dict(), 'FFNN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
