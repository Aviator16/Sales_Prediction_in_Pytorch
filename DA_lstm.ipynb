{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83739a3d",
   "metadata": {},
   "source": [
    "## Importing libraries and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec64270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f9b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            store  item  sales\n",
       "date                          \n",
       "2013-01-01      1     1     13\n",
       "2013-01-02      1     1     11\n",
       "2013-01-03      1     1     14\n",
       "2013-01-04      1     1     13\n",
       "2013-01-05      1     1     10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', parse_dates=['date'], index_col = ['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd39674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store  item  sales\n",
       "0 2013-01-01      1     1     13\n",
       "1 2013-01-02      1     1     11\n",
       "2 2013-01-03      1     1     14\n",
       "3 2013-01-04      1     1     13\n",
       "4 2013-01-05      1     1     10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df[df.index.year == 2017]\n",
    "test.reset_index(level=0, inplace= True)\n",
    "train = df[df.index.year != 2017]\n",
    "train.reset_index(level = 0, inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459ff05",
   "metadata": {},
   "source": [
    "# Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b45341",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'year': train['date'].dt.year-2013, 'month': train['date'].dt.month,\n",
    "                           'day': train['date'].dt.day, 'weekday': train['date'].dt.weekday,\n",
    "                           'store': train['store'], 'item': train['item'], 'sales': train['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n",
    "\n",
    "test_data = pd.DataFrame({'year': test['date'].dt.year-2013, 'month': test['date'].dt.month,\n",
    "                           'day': test['date'].dt.day, 'weekday': test['date'].dt.weekday,\n",
    "                           'store': test['store'], 'item': test['item'], 'sales': test['sales']},\n",
    "                          columns =['year', 'month', 'day', 'weekday', 'store', 'item', 'sales'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc87eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month  day  weekday  store  item  sales\n",
      "0     0      1    1        1      1     1     13\n",
      "1     0      1    2        2      1     1     11\n",
      "2     0      1    3        3      1     1     14\n",
      "3     0      1    4        4      1     1     13\n",
      "4     0      1    5        5      1     1     10\n",
      "(730500, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73fb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_data.drop('sales', axis = 1))\n",
    "y = np.array(train_data['sales'])\n",
    "X_test = np.array(test_data.drop('sales', axis = 1))\n",
    "y_test = np.array(test_data['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d91b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X_train, y_train, val_ratio = 0.2, val_year = 3, half_yearly = 1, randomly = True):\n",
    "    \n",
    "    # Splitting randomly\n",
    "    if randomly:\n",
    "        X_tr, y_tr, X_val, y_val = train_test_split(X_train, y_train, test_size = (val_ratio),\n",
    "                                                          random_state = 6, shuffle = True)\n",
    "    else:\n",
    "        if half_yearly == 1:        #if validation data is first 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]   #if not val_year or in last 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]>6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)] #if val_year and first 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]<=6)]\n",
    "            \n",
    "        else:                       #if validation data is last 6 months of val_year\n",
    "            \n",
    "            X_tr = X_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]  #if not val_year or in first 6 months of year\n",
    "            y_tr = y_train[(X_train[:,0]!=val_year) | (X_train[:,1]<=6)]\n",
    "            \n",
    "            X_val = X_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]  #if val_year and last 6 months of year\n",
    "            y_val = y_train[(X_train[:,0]==val_year) & (X_train[:,1]>6)]\n",
    "            \n",
    "        return X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc559b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (730500, 6) (730500,)\n",
      "Validation: (0, 6) (0,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = split_data(X, y, False, 0.3, 3, 0)\n",
    "print(\"Training:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe1d3b",
   "metadata": {},
   "source": [
    "# Creating Long Short Term Memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899a8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 13, 32, 8, 11, 51]\n",
      "[(5, 3), (13, 7), (32, 16), (8, 4), (11, 6), (51, 26)]\n"
     ]
    }
   ],
   "source": [
    "# For embeddings, the thumb rule is, num_embeddings = no. of unique valus in category + 1 \n",
    "# & embedding_dim = min(50,feat_dim(num_embeddings)/2)\n",
    "dims = [np.unique(X_train[:,i]).size+1 for i in range(X_train.shape[1])]\n",
    "print(dims)\n",
    "embedding_dim = [(x, min(50, (x+1)//2)) for x in dims]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896e1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class for LSTM\n",
    "class LSTMwithEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_cont, out_size, layers, dp = 0.3): #n_cont=no. of cont. feat. in dataframe\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        #print(self.embeds)\n",
    "        self.emb_drop = nn.Dropout(dp)\n",
    "        \n",
    "        layer_list = []\n",
    "        n_emb = sum((out for inp,out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "        n_hidden = [112, 96]\n",
    "        n_dense = [64, 16]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(n_in, n_hidden[0], 1, batch_first = True) #(no. of inputs, hidden_size, num_layers)\n",
    "        self.lstm2 = nn.LSTM(n_hidden[0], n_hidden[1], 1, batch_first = True)\n",
    "        n_in_dense = n_hidden[1]    \n",
    "        for i in n_dense:\n",
    "            layer_list.append(nn.Linear(n_in_dense, i))\n",
    "            layer_list.append(nn.ReLU(inplace = True))\n",
    "            #layer_list.append(nn.BatchNorm1d(i))\n",
    "            n_in_dense = i\n",
    "            \n",
    "        layer_list.append(nn.Dropout(dp))\n",
    "        layer_list.append(nn.Linear(n_in_dense, out_size))\n",
    "\n",
    "        self.dense_layers = nn.Sequential(*layer_list)\n",
    "\n",
    "\n",
    "    def forward(self, X_cat, X_cont):\n",
    "        embeddings = []\n",
    "        n_hidden = [112, 96]\n",
    "        batch_size = 365\n",
    "        seq_len = 1\n",
    "        for i, e in enumerate(self.embeds):\n",
    "            embeddings.append(e(X_cat[:,i]))\n",
    "        X = torch.cat(embeddings, axis =1)\n",
    "        X = self.emb_drop(X)\n",
    "        X = torch.cat([X, torch.unsqueeze(X_cont,1)], 1)\n",
    "        \n",
    "        h_1 = torch.randn(1, X.size(0), n_hidden[0]) #hidden state  ##(num_layers, batch_size, hidden_size)\n",
    "        c_1 = torch.randn(1, X.size(0), n_hidden[0]) #internal state/cell state\n",
    "        \n",
    "        h_2 = torch.randn(1, X.size(0), n_hidden[1]) #hidden state\n",
    "        c_2 = torch.randn(1, X.size(0), n_hidden[1]) #internal state/cell state\n",
    "        \n",
    "        h_out1, c_out1 = self.lstm1(X.reshape(X.size(0), 1, X.size(1)), (h_1,c_1)) #((batch_size(no. of elements in batch),seq_len, input_shape),(hidden))\n",
    "        h_out2, c_out2 = self.lstm2(h_out1, (h_2,c_2))\n",
    "        \n",
    "        out = self.dense_layers(h_out2)\n",
    "        return out.reshape(out.size(0),-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8649dcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMwithEmbeddings(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(13, 7)\n",
       "    (1): Embedding(32, 16)\n",
       "    (2): Embedding(8, 4)\n",
       "    (3): Embedding(11, 6)\n",
       "    (4): Embedding(51, 26)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0, inplace=False)\n",
       "  (lstm1): LSTM(60, 112, batch_first=True)\n",
       "  (lstm2): LSTM(112, 96, batch_first=True)\n",
       "  (dense_layers): Sequential(\n",
       "    (0): Linear(in_features=96, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Dropout(p=0, inplace=False)\n",
       "    (5): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiating LSTM model with 2 LSTM layers, 2 Fully connected layers and an output layer \n",
    "# having 112,96,64 & 16 nodes respectively\n",
    "\n",
    "LSTMmodel = LSTMwithEmbeddings(embedding_dim[1:], 1, 1,2, 0)\n",
    "LSTMmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0165a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function (for optional use)\n",
    "def smape(x,y):\n",
    "    return 100*torch.mean(2*torch.abs(x-y)/(torch.abs(x)+torch.abs(y)))\n",
    "\n",
    "optim = torch.optim.Adam(LSTMmodel.parameters(), lr = 0.01)\n",
    "lossfn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450725e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f50a4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, sets, model, X_train, y_train, lossfn, optimizer):\n",
    "    for i in range(sets//2):\n",
    "        for j in range(2):\n",
    "            X_tr, y_tr, X_val, y_val = split_data(X_train, y_train, val_year=i, half_yearly=j, randomly=False)\n",
    "            losses = []\n",
    "            for k in range(epochs):\n",
    "                k+=1\n",
    "                y_pred = LSTMmodel(torch.from_numpy(X_tr[:,1:]), torch.from_numpy(X_tr[:,0]))\n",
    "                y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n",
    "                loss = lossfn(y_pred,torch.tensor(y_tr))\n",
    "                losses.append(loss)\n",
    "                #if k%2 == 1:\n",
    "                print(\"Epoch number {} of validation year 20{} and half {} has MSE loss {}\".format(k,13+i,j,loss.item()))\n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if k%2 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        yhat_val = LSTMmodel(torch.from_numpy(X_val[:,1:]), torch.from_numpy(X_val[:,0]))\n",
    "                        y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n",
    "                        val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n",
    "                    print(\"Validation loss at epoch {} of year 20{} half {} is {:.4f}\".format(k,13+i,j,val_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ffa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833d5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6182/3471745253.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = lossfn(y_pred,torch.tensor(y_tr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1 of validation year 2013 and half 0 has MSE loss 3464.2373046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6182/3471745253.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tr = torch.tensor(y_tr,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 2 of validation year 2013 and half 0 has MSE loss 3456.761474609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6182/3471745253.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_loss = torch.sqrt(lossfn(torch.tensor(y_val), yhat_val))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 2 of year 2013 half 0 is 51.3380\n",
      "Epoch number 3 of validation year 2013 and half 0 has MSE loss 3444.312255859375\n",
      "Epoch number 4 of validation year 2013 and half 0 has MSE loss 3419.999267578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6182/3471745253.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val,dtype=torch.float).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss at epoch 4 of year 2013 half 0 is 50.7352\n",
      "Epoch number 5 of validation year 2013 and half 0 has MSE loss 3368.896240234375\n",
      "Epoch number 6 of validation year 2013 and half 0 has MSE loss 3264.26953125\n",
      "Validation loss at epoch 6 of year 2013 half 0 is 48.3136\n",
      "Epoch number 7 of validation year 2013 and half 0 has MSE loss 3067.145263671875\n",
      "Epoch number 8 of validation year 2013 and half 0 has MSE loss 2738.499755859375\n",
      "Validation loss at epoch 8 of year 2013 half 0 is 41.0697\n",
      "Epoch number 9 of validation year 2013 and half 0 has MSE loss 2262.511474609375\n",
      "Epoch number 10 of validation year 2013 and half 0 has MSE loss 1677.7618408203125\n",
      "Validation loss at epoch 10 of year 2013 half 0 is 27.8425\n",
      "Epoch number 11 of validation year 2013 and half 0 has MSE loss 1113.8936767578125\n",
      "Epoch number 12 of validation year 2013 and half 0 has MSE loss 809.1383056640625\n",
      "Validation loss at epoch 12 of year 2013 half 0 is 30.5546\n",
      "Epoch number 13 of validation year 2013 and half 0 has MSE loss 1009.9547729492188\n",
      "Epoch number 14 of validation year 2013 and half 0 has MSE loss 1315.8707275390625\n",
      "Validation loss at epoch 14 of year 2013 half 0 is 35.5563\n",
      "Epoch number 15 of validation year 2013 and half 0 has MSE loss 1267.79296875\n",
      "Epoch number 1 of validation year 2013 and half 1 has MSE loss 1015.1442260742188\n",
      "Epoch number 2 of validation year 2013 and half 1 has MSE loss 827.6298217773438\n",
      "Validation loss at epoch 2 of year 2013 half 1 is 24.6298\n",
      "Epoch number 3 of validation year 2013 and half 1 has MSE loss 756.6356811523438\n",
      "Epoch number 4 of validation year 2013 and half 1 has MSE loss 784.2695922851562\n",
      "Validation loss at epoch 4 of year 2013 half 1 is 22.1249\n",
      "Epoch number 5 of validation year 2013 and half 1 has MSE loss 853.0584106445312\n",
      "Epoch number 6 of validation year 2013 and half 1 has MSE loss 913.0285034179688\n",
      "Validation loss at epoch 6 of year 2013 half 1 is 22.3538\n",
      "Epoch number 7 of validation year 2013 and half 1 has MSE loss 938.934326171875\n",
      "Epoch number 8 of validation year 2013 and half 1 has MSE loss 924.3773803710938\n",
      "Validation loss at epoch 8 of year 2013 half 1 is 21.7479\n",
      "Epoch number 9 of validation year 2013 and half 1 has MSE loss 875.6373901367188\n",
      "Epoch number 10 of validation year 2013 and half 1 has MSE loss 806.9559936523438\n",
      "Validation loss at epoch 10 of year 2013 half 1 is 21.2767\n",
      "Epoch number 11 of validation year 2013 and half 1 has MSE loss 736.6889038085938\n",
      "Epoch number 12 of validation year 2013 and half 1 has MSE loss 683.32275390625\n",
      "Validation loss at epoch 12 of year 2013 half 1 is 23.0785\n",
      "Epoch number 13 of validation year 2013 and half 1 has MSE loss 657.8120727539062\n",
      "Epoch number 14 of validation year 2013 and half 1 has MSE loss 656.6123657226562\n",
      "Validation loss at epoch 14 of year 2013 half 1 is 25.6434\n",
      "Epoch number 15 of validation year 2013 and half 1 has MSE loss 661.8441772460938\n",
      "Epoch number 1 of validation year 2014 and half 0 has MSE loss 664.8255615234375\n",
      "Epoch number 2 of validation year 2014 and half 0 has MSE loss 629.5084838867188\n",
      "Validation loss at epoch 2 of year 2014 half 0 is 23.1600\n",
      "Epoch number 3 of validation year 2014 and half 0 has MSE loss 582.05029296875\n",
      "Epoch number 4 of validation year 2014 and half 0 has MSE loss 544.72216796875\n",
      "Validation loss at epoch 4 of year 2014 half 0 is 23.2486\n",
      "Epoch number 5 of validation year 2014 and half 0 has MSE loss 528.7655639648438\n",
      "Epoch number 6 of validation year 2014 and half 0 has MSE loss 523.6950073242188\n",
      "Validation loss at epoch 6 of year 2014 half 0 is 23.5207\n",
      "Epoch number 7 of validation year 2014 and half 0 has MSE loss 511.7267150878906\n",
      "Epoch number 8 of validation year 2014 and half 0 has MSE loss 484.19659423828125\n",
      "Validation loss at epoch 8 of year 2014 half 0 is 21.7202\n",
      "Epoch number 9 of validation year 2014 and half 0 has MSE loss 447.46710205078125\n",
      "Epoch number 10 of validation year 2014 and half 0 has MSE loss 413.7348327636719\n",
      "Validation loss at epoch 10 of year 2014 half 0 is 19.4890\n",
      "Epoch number 11 of validation year 2014 and half 0 has MSE loss 391.1456298828125\n",
      "Epoch number 12 of validation year 2014 and half 0 has MSE loss 380.217041015625\n",
      "Validation loss at epoch 12 of year 2014 half 0 is 18.3679\n",
      "Epoch number 13 of validation year 2014 and half 0 has MSE loss 373.4756774902344\n",
      "Epoch number 14 of validation year 2014 and half 0 has MSE loss 363.4500427246094\n",
      "Validation loss at epoch 14 of year 2014 half 0 is 17.5721\n",
      "Epoch number 15 of validation year 2014 and half 0 has MSE loss 345.9429016113281\n",
      "Epoch number 1 of validation year 2014 and half 1 has MSE loss 324.403564453125\n",
      "Epoch number 2 of validation year 2014 and half 1 has MSE loss 306.19305419921875\n",
      "Validation loss at epoch 2 of year 2014 half 1 is 15.7294\n",
      "Epoch number 3 of validation year 2014 and half 1 has MSE loss 292.65338134765625\n",
      "Epoch number 4 of validation year 2014 and half 1 has MSE loss 281.5181579589844\n",
      "Validation loss at epoch 4 of year 2014 half 1 is 14.9148\n",
      "Epoch number 5 of validation year 2014 and half 1 has MSE loss 269.310302734375\n",
      "Epoch number 6 of validation year 2014 and half 1 has MSE loss 254.34475708007812\n",
      "Validation loss at epoch 6 of year 2014 half 1 is 14.2865\n",
      "Epoch number 7 of validation year 2014 and half 1 has MSE loss 239.61317443847656\n",
      "Epoch number 8 of validation year 2014 and half 1 has MSE loss 227.3802032470703\n",
      "Validation loss at epoch 8 of year 2014 half 1 is 14.0428\n",
      "Epoch number 9 of validation year 2014 and half 1 has MSE loss 219.61837768554688\n",
      "Epoch number 10 of validation year 2014 and half 1 has MSE loss 212.77349853515625\n",
      "Validation loss at epoch 10 of year 2014 half 1 is 13.6731\n",
      "Epoch number 11 of validation year 2014 and half 1 has MSE loss 204.0682373046875\n",
      "Epoch number 12 of validation year 2014 and half 1 has MSE loss 194.2311248779297\n",
      "Validation loss at epoch 12 of year 2014 half 1 is 12.8185\n",
      "Epoch number 13 of validation year 2014 and half 1 has MSE loss 185.29859924316406\n",
      "Epoch number 14 of validation year 2014 and half 1 has MSE loss 178.3800811767578\n",
      "Validation loss at epoch 14 of year 2014 half 1 is 12.2600\n",
      "Epoch number 15 of validation year 2014 and half 1 has MSE loss 171.66461181640625\n",
      "Epoch number 1 of validation year 2015 and half 0 has MSE loss 160.0783233642578\n",
      "Epoch number 2 of validation year 2015 and half 0 has MSE loss 153.2339630126953\n",
      "Validation loss at epoch 2 of year 2015 half 0 is 12.3300\n",
      "Epoch number 3 of validation year 2015 and half 0 has MSE loss 148.06155395507812\n",
      "Epoch number 4 of validation year 2015 and half 0 has MSE loss 143.5914306640625\n",
      "Validation loss at epoch 4 of year 2015 half 0 is 11.8843\n",
      "Epoch number 5 of validation year 2015 and half 0 has MSE loss 139.4907989501953\n",
      "Epoch number 6 of validation year 2015 and half 0 has MSE loss 134.64999389648438\n",
      "Validation loss at epoch 6 of year 2015 half 0 is 11.5825\n",
      "Epoch number 7 of validation year 2015 and half 0 has MSE loss 130.2216339111328\n",
      "Epoch number 8 of validation year 2015 and half 0 has MSE loss 126.11605834960938\n",
      "Validation loss at epoch 8 of year 2015 half 0 is 11.2684\n",
      "Epoch number 9 of validation year 2015 and half 0 has MSE loss 122.0167007446289\n",
      "Epoch number 10 of validation year 2015 and half 0 has MSE loss 117.50467681884766\n",
      "Validation loss at epoch 10 of year 2015 half 0 is 10.8071\n",
      "Epoch number 11 of validation year 2015 and half 0 has MSE loss 113.71366882324219\n",
      "Epoch number 12 of validation year 2015 and half 0 has MSE loss 110.73283386230469\n",
      "Validation loss at epoch 12 of year 2015 half 0 is 10.5055\n",
      "Epoch number 13 of validation year 2015 and half 0 has MSE loss 107.74565124511719\n",
      "Epoch number 14 of validation year 2015 and half 0 has MSE loss 104.9278335571289\n",
      "Validation loss at epoch 14 of year 2015 half 0 is 10.2914\n",
      "Epoch number 15 of validation year 2015 and half 0 has MSE loss 102.32894897460938\n",
      "Epoch number 1 of validation year 2015 and half 1 has MSE loss 101.15752410888672\n",
      "Epoch number 2 of validation year 2015 and half 1 has MSE loss 98.86870574951172\n",
      "Validation loss at epoch 2 of year 2015 half 1 is 9.5573\n",
      "Epoch number 3 of validation year 2015 and half 1 has MSE loss 96.45636749267578\n",
      "Epoch number 4 of validation year 2015 and half 1 has MSE loss 94.76252746582031\n",
      "Validation loss at epoch 4 of year 2015 half 1 is 9.4584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 5 of validation year 2015 and half 1 has MSE loss 93.20987701416016\n",
      "Epoch number 6 of validation year 2015 and half 1 has MSE loss 91.51852416992188\n",
      "Validation loss at epoch 6 of year 2015 half 1 is 9.2564\n",
      "Epoch number 7 of validation year 2015 and half 1 has MSE loss 90.13162231445312\n",
      "Epoch number 8 of validation year 2015 and half 1 has MSE loss 88.64959716796875\n",
      "Validation loss at epoch 8 of year 2015 half 1 is 9.0840\n",
      "Epoch number 9 of validation year 2015 and half 1 has MSE loss 87.43643951416016\n",
      "Epoch number 10 of validation year 2015 and half 1 has MSE loss 85.87007141113281\n",
      "Validation loss at epoch 10 of year 2015 half 1 is 8.9370\n",
      "Epoch number 11 of validation year 2015 and half 1 has MSE loss 84.58817291259766\n",
      "Epoch number 12 of validation year 2015 and half 1 has MSE loss 83.570068359375\n",
      "Validation loss at epoch 12 of year 2015 half 1 is 8.8025\n",
      "Epoch number 13 of validation year 2015 and half 1 has MSE loss 82.35394287109375\n",
      "Epoch number 14 of validation year 2015 and half 1 has MSE loss 81.24794006347656\n",
      "Validation loss at epoch 14 of year 2015 half 1 is 8.6652\n",
      "Epoch number 15 of validation year 2015 and half 1 has MSE loss 80.1775894165039\n",
      "Epoch number 1 of validation year 2016 and half 0 has MSE loss 75.715576171875\n",
      "Epoch number 2 of validation year 2016 and half 0 has MSE loss 74.92254638671875\n",
      "Validation loss at epoch 2 of year 2016 half 0 is 9.7748\n",
      "Epoch number 3 of validation year 2016 and half 0 has MSE loss 74.01530456542969\n",
      "Epoch number 4 of validation year 2016 and half 0 has MSE loss 73.16376495361328\n",
      "Validation loss at epoch 4 of year 2016 half 0 is 9.6777\n",
      "Epoch number 5 of validation year 2016 and half 0 has MSE loss 72.55506134033203\n",
      "Epoch number 6 of validation year 2016 and half 0 has MSE loss 71.7080307006836\n",
      "Validation loss at epoch 6 of year 2016 half 0 is 9.6173\n",
      "Epoch number 7 of validation year 2016 and half 0 has MSE loss 71.13845825195312\n",
      "Epoch number 8 of validation year 2016 and half 0 has MSE loss 70.5117416381836\n",
      "Validation loss at epoch 8 of year 2016 half 0 is 9.5458\n",
      "Epoch number 9 of validation year 2016 and half 0 has MSE loss 69.98712158203125\n",
      "Epoch number 10 of validation year 2016 and half 0 has MSE loss 69.31570434570312\n",
      "Validation loss at epoch 10 of year 2016 half 0 is 9.4378\n",
      "Epoch number 11 of validation year 2016 and half 0 has MSE loss 68.84744262695312\n",
      "Epoch number 12 of validation year 2016 and half 0 has MSE loss 68.5181655883789\n",
      "Validation loss at epoch 12 of year 2016 half 0 is 9.4038\n",
      "Epoch number 13 of validation year 2016 and half 0 has MSE loss 67.86566925048828\n",
      "Epoch number 14 of validation year 2016 and half 0 has MSE loss 67.41213989257812\n",
      "Validation loss at epoch 14 of year 2016 half 0 is 9.3234\n",
      "Epoch number 15 of validation year 2016 and half 0 has MSE loss 66.96337127685547\n",
      "Epoch number 1 of validation year 2016 and half 1 has MSE loss 68.2838134765625\n",
      "Epoch number 2 of validation year 2016 and half 1 has MSE loss 67.76986694335938\n",
      "Validation loss at epoch 2 of year 2016 half 1 is 8.5780\n",
      "Epoch number 3 of validation year 2016 and half 1 has MSE loss 67.54621887207031\n",
      "Epoch number 4 of validation year 2016 and half 1 has MSE loss 67.00708770751953\n",
      "Validation loss at epoch 4 of year 2016 half 1 is 8.5093\n",
      "Epoch number 5 of validation year 2016 and half 1 has MSE loss 66.57244110107422\n",
      "Epoch number 6 of validation year 2016 and half 1 has MSE loss 66.32562255859375\n",
      "Validation loss at epoch 6 of year 2016 half 1 is 8.4782\n",
      "Epoch number 7 of validation year 2016 and half 1 has MSE loss 65.89024353027344\n",
      "Epoch number 8 of validation year 2016 and half 1 has MSE loss 65.6107177734375\n",
      "Validation loss at epoch 8 of year 2016 half 1 is 8.4200\n",
      "Epoch number 9 of validation year 2016 and half 1 has MSE loss 65.24925231933594\n",
      "Epoch number 10 of validation year 2016 and half 1 has MSE loss 65.04312896728516\n",
      "Validation loss at epoch 10 of year 2016 half 1 is 8.3759\n",
      "Epoch number 11 of validation year 2016 and half 1 has MSE loss 64.72221374511719\n",
      "Epoch number 12 of validation year 2016 and half 1 has MSE loss 64.35148620605469\n",
      "Validation loss at epoch 12 of year 2016 half 1 is 8.3370\n",
      "Epoch number 13 of validation year 2016 and half 1 has MSE loss 64.16083526611328\n",
      "Epoch number 14 of validation year 2016 and half 1 has MSE loss 63.72991943359375\n",
      "Validation loss at epoch 14 of year 2016 half 1 is 8.3016\n",
      "Epoch number 15 of validation year 2016 and half 1 has MSE loss 63.52240753173828\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "fit(15, 8, LSTMmodel, X, y, lossfn, optim)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "744d2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training in a Ryzen 5 Hexa core 4600H CPU is 42.62 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken for training in a Ryzen 5 Hexa core 4600H CPU is {:.2f} minutes\".format((end-begin)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6ed21",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c036cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "#import torch_metrics as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bdd7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(test, y_pred_final):\n",
    "    metrics = {'R2_score': r2_score(test, y_pred_final), 'MAE': mean_absolute_error(test, y_pred_final),\n",
    "               'RMSE': mean_squared_error(test, y_pred_final, squared=False),\n",
    "               'MAPE': mean_absolute_percentage_error(test, y_pred_final)}\n",
    "    adj_R2 = 1-(1-metrics['R2_score'])*(len(test)-1)/(len(test)-6-1)      #num of indep var = 6\n",
    "    metrics['adj_R2'] = adj_R2\n",
    "    print(\"R2 score on test set is\", metrics['R2_score'])\n",
    "    print(\"Mean Absolute Error on test set is\", metrics['MAE'])\n",
    "    print(\"Root Mean Square error on test set is\", metrics['RMSE'])\n",
    "    print(\"Mean Absolute Percentage Error on test set is\", metrics['MAPE'])\n",
    "    print(\"Adjusted R2 score on test set is\", adj_R2)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52181b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on test set is 0.9130831700643351\n",
      "Mean Absolute Error on test set is 7.067161381278626\n",
      "Root Mean Square error on test set is 9.302233612700073\n",
      "Mean Absolute Percentage Error on test set is 0.15067687205231767\n",
      "Adjusted R2 score on test set is 0.9130803124151123\n"
     ]
    }
   ],
   "source": [
    "predictions = LSTMmodel(torch.from_numpy(X_test[:,1:]), torch.from_numpy(X_test[:,0]))\n",
    "test_results = show_metrics(y_test, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb10a8c",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9742295",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(LSTMmodel.state_dict(), 'LSTM.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
